{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d830ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gokalp\\.conda\\envs\\myworksv2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# 0) Imports & Global Configuration\n",
    "# ===============================================================\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.stats as st\n",
    "from scipy.stats import friedmanchisquare, wilcoxon\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "import optuna\n",
    "from joblib import dump\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import (\n",
    "    f1_score, roc_auc_score, average_precision_score,\n",
    "    confusion_matrix, roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# ------------------ Global constants ------------------\n",
    "RANDOM_BASE = 42\n",
    "N_REPS = 20\n",
    "\n",
    "# 65/15/20 global split -> inner (train,val) over trainval(80%)\n",
    "TEST_FRACTION = 0.20\n",
    "VAL_FRACTION_GLOBAL = 0.15\n",
    "val_in_trainval   = VAL_FRACTION_GLOBAL / (1.0 - TEST_FRACTION)   # 0.1875\n",
    "train_in_trainval = 1.0 - val_in_trainval                         # 0.8125\n",
    "\n",
    "# Feature selection bounds (k-Best)\n",
    "K_MIN = 3\n",
    "K_MAX = 20\n",
    "\n",
    "# Optuna target: \"ap\" (PR-AUC), \"f1\", or \"auc\"\n",
    "OPTIMIZE_FOR = \"f1\"   # set \"ap\" as requested (can be \"f1\"/\"auc\")\n",
    "\n",
    "# Optuna trials per model\n",
    "N_TRIALS_BY_MODEL = {\n",
    "    \"Random Forest\": 150,\n",
    "    \"SVM\":           150,\n",
    "    \"MLP\":           150,\n",
    "    \"XGBoost\":       150,\n",
    "    \"KNN\":           150,\n",
    "    \"Bagging\":       150,\n",
    "}\n",
    "\n",
    "# ===============================================================\n",
    "#  UPDATED PATHS FOR NEW DIRECTORY\n",
    "# ===============================================================\n",
    "\n",
    "# Root folder\n",
    "BASE_OUT_DIR = Path(r\"C:\\Users\\gokalp\\Desktop\\PTC\")\n",
    "\n",
    "# Subdirectories (created automatically)\n",
    "RUN_TAG      = f\"OBJ_{OPTIMIZE_FOR}\"\n",
    "MODEL_DIR    = BASE_OUT_DIR / f\"models_{RUN_TAG}\"\n",
    "OUT_DIR      = BASE_OUT_DIR / f\"outputs_{RUN_TAG}\"\n",
    "EXPL_DIR     = BASE_OUT_DIR / f\"explanations_plots_{RUN_TAG}\"\n",
    "LIME_DIR     = EXPL_DIR / \"lime_html\"\n",
    "STATS_PLOTS  = BASE_OUT_DIR / f\"stats_plots_{RUN_TAG}\"\n",
    "\n",
    "for d in [BASE_OUT_DIR, MODEL_DIR, OUT_DIR, EXPL_DIR, LIME_DIR, STATS_PLOTS]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ===============================================================\n",
    "#  DATA PATH (EXCEL FILE)\n",
    "# ===============================================================\n",
    "\n",
    "DATA_XLS_PATH = r\"C:\\Users\\gokalp\\Desktop\\PTC\\PTC.xlsx\"\n",
    "\n",
    "# IMPORTANT:\n",
    "# \"Group\" variable is the FIRST COLUMN in the dataset\n",
    "# The pipeline will treat df.iloc[:, 0] as y\n",
    "\n",
    "SAVE_MODELS = True\n",
    "DO_EDA_PLOTS = True\n",
    "DO_EXPLAINERS = True\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d879237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# 1) Load Data\n",
    "# ===============================================================\n",
    "data = pd.read_excel(DATA_XLS_PATH)\n",
    "\n",
    "# ---------------------- IMPORTANT --------------------------\n",
    "# Your dataset: FIRST COLUMN is the label (\"Group\")\n",
    "# You previously said 0 = Control, 1 = Patient\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "TARGET_COL = data.columns[0]   # ensures first column is target\n",
    "if TARGET_COL.lower() not in [\"group\", \"class\", \"target\"]:\n",
    "    warnings.warn(\n",
    "        f\"First column detected as '{TARGET_COL}'. \"\n",
    "        \"Using this as target because you stated the label is in the first column.\"\n",
    "    )\n",
    "\n",
    "y = data.iloc[:, 0].values     # first column = target\n",
    "X = data.iloc[:, 1:]           # all remaining = features\n",
    "\n",
    "# ---------------------- Check Binary -------------------------\n",
    "classes_sorted = np.unique(y)\n",
    "if len(classes_sorted) != 2:\n",
    "    raise ValueError(\n",
    "        f\"Binary classification expected; got {len(classes_sorted)} classes instead: {classes_sorted}\"\n",
    "    )\n",
    "\n",
    "neg_label, pos_label = classes_sorted[0], classes_sorted[1]\n",
    "print(f\"Label column: {TARGET_COL} | Negative={neg_label} | Positive={pos_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62535738",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_map = X.isna()\n",
    "\n",
    "# Sadece NaN olan satırları göster\n",
    "nan_rows = nan_map[nan_map.any(axis=1)]\n",
    "\n",
    "print(\"\\n=== Rows Containing NaN Values ===\")\n",
    "print(nan_rows)\n",
    "\n",
    "print(\"\\n=== NaN Count Per Column ===\")\n",
    "print(X.isna().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3f572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# 2) Preprocessing (leakage-free in Pipeline)\n",
    "# ===============================================================\n",
    "\n",
    "def build_preprocessor_all_robust(X_train: pd.DataFrame) -> ColumnTransformer:\n",
    "    \"\"\"\n",
    "    RobustScaler is applied to ALL feature columns.\n",
    "    Fitting is done INSIDE the pipeline to avoid data leakage.\n",
    "    \"\"\"\n",
    "    feature_cols = list(X_train.columns)\n",
    "    return ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('rob_all', RobustScaler(), feature_cols)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "\n",
    "\n",
    "def clean_names(names):\n",
    "    \"\"\"\n",
    "    Remove transformer prefixes from pipeline-transformed feature names.\n",
    "    Example:\n",
    "        'rob_all__ONSD_R' → 'ONSD_R'\n",
    "    This ensures readable feature names in:\n",
    "        - feature importance tables\n",
    "        - Optuna logs\n",
    "        - saved outputs\n",
    "        - LIME explanations\n",
    "    \"\"\"\n",
    "    cleaned = []\n",
    "    for n in names:\n",
    "        if \"__\" in n:\n",
    "            cleaned.append(n.split(\"__\", 1)[1])\n",
    "        else:\n",
    "            cleaned.append(n)\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275365b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# 3) Optional EDA (correlations & group-wise boxplots)\n",
    "# ===============================================================\n",
    "if DO_EDA_PLOTS:\n",
    "    try:\n",
    "        # ---------- Correlation Heatmap ----------\n",
    "        num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "        corr = X[num_cols].corr()\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=.5)\n",
    "        plt.title(\"Correlation Heatmap of MRI-Derived Quantitative Features\", \n",
    "                  fontsize=16, fontweight='semibold')\n",
    "        plt.xlabel(\"Features\", fontsize=13)\n",
    "        plt.ylabel(\"Features\", fontsize=13)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUT_DIR / \"eda_correlation_heatmap.png\", dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "        # ---------- Group-wise Boxplots ----------\n",
    "        feats = [col for col in num_cols if col.lower() != \"sex\"]\n",
    "        if len(feats) > 0:\n",
    "            n = len(feats)\n",
    "            n_cols = 3\n",
    "            n_rows = int(np.ceil(n / n_cols))\n",
    "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 3.5*n_rows))\n",
    "            axes = axes.ravel() if n_rows * n_cols > 1 else [axes]\n",
    "\n",
    "            last_i = 0\n",
    "            for i, col in enumerate(feats):\n",
    "                ax = axes[i]\n",
    "\n",
    "                # Group definitions\n",
    "                d0 = data.loc[data.iloc[:, 0] == neg_label, col].dropna()   # Control\n",
    "                d1 = data.loc[data.iloc[:, 0] == pos_label, col].dropna()   # Pediatric IIH\n",
    "\n",
    "                # Boxplot\n",
    "                ax.boxplot(\n",
    "                    [d0, d1],\n",
    "                    labels=[\"Control\", \"Pediatric IIH\"],\n",
    "                    showfliers=True,\n",
    "                    patch_artist=True,\n",
    "                    boxprops=dict(facecolor='#D0E4F5')\n",
    "                )\n",
    "\n",
    "                # Title & labels\n",
    "                ax.set_title(col, fontsize=12, fontweight='semibold')\n",
    "                ax.set_ylabel(\"Feature Value\", fontsize=14)\n",
    "                ax.tick_params(axis='x', labelsize=12)\n",
    "                ax.tick_params(axis='y', labelsize=14)\n",
    "                ax.grid(axis='y', linestyle=':', linewidth=0.4)\n",
    "\n",
    "                # Tick styling\n",
    "                for tick in ax.get_xticklabels():\n",
    "                    tick.set_fontweight('bold')\n",
    "                    tick.set_fontsize(12)\n",
    "                    tick.set_color('#2B2B2B')\n",
    "\n",
    "                last_i = i\n",
    "\n",
    "            # Disable unused subplots\n",
    "            for j in range(last_i + 1, len(axes)):\n",
    "                axes[j].axis(\"off\")\n",
    "\n",
    "            # Main figure title\n",
    "            fig.suptitle(\n",
    "                \"Distribution of MRI-Based Quantitative Features in Pediatric IIH vs Controls\",\n",
    "                y=1.02,\n",
    "                fontsize=22,\n",
    "                fontweight='bold'\n",
    "            )\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(OUT_DIR / \"eda_boxplots_by_group.png\", dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"EDA plotting skipped due to error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aecb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# 4) Utility Functions (metrics, CI, stats, plotting helpers)\n",
    "# ===============================================================\n",
    "\n",
    "def safe_div(a, b):\n",
    "    return np.nan if b == 0 else a / b\n",
    "\n",
    "def compute_row_metrics(tn, fp, fn, tp):\n",
    "    tn = float(tn); fp = float(fp); fn = float(fn); tp = float(tp)\n",
    "    sens = safe_div(tp, tp + fn)\n",
    "    spec = safe_div(tn, tn + fp)\n",
    "    prec = safe_div(tp, tp + fp)\n",
    "    rec  = sens\n",
    "    f1 = np.nan if (np.isnan(prec) or np.isnan(rec) or (prec + rec) == 0) else (2 * prec * rec / (prec + rec))\n",
    "    denom_mcc = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
    "    mcc = np.nan if denom_mcc <= 0 else ((tp * tn) - (fp * fn)) / np.sqrt(denom_mcc)\n",
    "    gmean = np.nan if (np.isnan(sens) or np.isnan(spec)) else np.sqrt(sens * spec)\n",
    "    total = tn + fp + fn + tp\n",
    "    acc = safe_div(tn + tp, total)\n",
    "    return sens, spec, prec, rec, f1, mcc, gmean, acc\n",
    "\n",
    "\n",
    "def mean_std_ci_t(x, alpha=0.05):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    x = x[~np.isnan(x)]\n",
    "    n = x.size\n",
    "    m = float(np.mean(x)) if n > 0 else np.nan\n",
    "    s = float(np.std(x, ddof=1)) if n > 1 else np.nan\n",
    "    if n > 1:\n",
    "        tcrit = st.t.ppf(1 - alpha/2, df=n-1)\n",
    "        half = tcrit * s / np.sqrt(n)\n",
    "        lo, hi = m - half, m + half\n",
    "    else:\n",
    "        lo, hi = np.nan, np.nan\n",
    "    return m, s, lo, hi, int(n)\n",
    "\n",
    "\n",
    "def paired_ttest_ci(a, b, alpha=0.05):\n",
    "    a = np.asarray(a, dtype=float)\n",
    "    b = np.asarray(b, dtype=float)\n",
    "    n_all = min(a.size, b.size)\n",
    "    a = a[:n_all]; b = b[:n_all]\n",
    "    mask = (~np.isnan(a)) & (~np.isnan(b))\n",
    "    a = a[mask]; b = b[mask]\n",
    "    n = a.size\n",
    "    if n <= 1:\n",
    "        return {'n': int(n), 'mean_diff': np.nan, 'ci95': (np.nan, np.nan), 't': np.nan, 'p': np.nan}\n",
    "    d = a - b\n",
    "    md = float(np.mean(d))\n",
    "    sd = float(np.std(d, ddof=1))\n",
    "    tstat, pval = st.ttest_1samp(d, popmean=0.0)\n",
    "    tcrit = st.t.ppf(1 - alpha/2, df=n - 1)\n",
    "    half = tcrit * sd / np.sqrt(n)\n",
    "    ci = (md - half, md + half)\n",
    "    return {'n': int(n), 'mean_diff': md, 'ci95': ci, 't': float(tstat), 'p': float(pval)}\n",
    "\n",
    "\n",
    "def friedman_and_pairwise(df_long, metric, alpha=0.05):\n",
    "    pivot = df_long.pivot_table(index='run', columns='model', values=metric)\n",
    "    pivot = pivot.dropna(axis=0, how='any')\n",
    "    models = list(pivot.columns)\n",
    "    if len(models) < 2 or pivot.shape[0] < 2:\n",
    "        empty = {'friedman': {'stat': np.nan, 'p': np.nan, 'k': len(models), 'n': pivot.shape[0]}}\n",
    "        return empty, pd.DataFrame(columns=['metric','model_a','model_b','p_raw','mean_diff','p_holm','better_model'])\n",
    "\n",
    "    stat, p = friedmanchisquare(*[pivot[m].values for m in models])\n",
    "    fried = {'stat': stat, 'p': p, 'k': len(models), 'n': pivot.shape[0]}\n",
    "\n",
    "    rows = []\n",
    "    for i in range(len(models)):\n",
    "        for j in range(i+1, len(models)):\n",
    "            a, b = models[i], models[j]\n",
    "            vals_a, vals_b = pivot[a].values, pivot[b].values\n",
    "            try:\n",
    "                _, p_raw = wilcoxon(vals_a, vals_b, zero_method='wilcox',\n",
    "                                    correction=False, alternative='two-sided', mode='auto')\n",
    "                diff_mean = float(np.mean(vals_a - vals_b))\n",
    "            except ValueError:\n",
    "                p_raw, diff_mean = np.nan, np.nan\n",
    "            rows.append([metric, a, b, p_raw, diff_mean])\n",
    "    pw = pd.DataFrame(rows, columns=['metric','model_a','model_b','p_raw','mean_diff'])\n",
    "\n",
    "    mask = ~pw['p_raw'].isna()\n",
    "    if mask.any():\n",
    "        _, p_holm, _, _ = multipletests(pw.loc[mask, 'p_raw'].values, method='holm')\n",
    "        pw.loc[mask, 'p_holm'] = p_holm\n",
    "    else:\n",
    "        pw['p_holm'] = np.nan\n",
    "\n",
    "    pw['better_model'] = np.where(\n",
    "        pw['mean_diff'] > 0, pw['model_a'],\n",
    "        np.where(pw['mean_diff'] < 0, pw['model_b'], 'equal')\n",
    "    )\n",
    "    return {'friedman': fried}, pw\n",
    "\n",
    "\n",
    "def normalize_cm(cm, mode=None):\n",
    "    cm = np.asarray(cm, dtype=float)\n",
    "    if mode is None:\n",
    "        return cm\n",
    "    if mode == \"row\":\n",
    "        row_sum = cm.sum(axis=1, keepdims=True)\n",
    "        row_sum[row_sum == 0] = 1.0\n",
    "        return cm / row_sum\n",
    "    if mode == \"all\":\n",
    "        s = cm.sum()\n",
    "        return cm / s if s > 0 else cm\n",
    "    raise ValueError(\"normalize mode must be None, 'row' or 'all'.\")\n",
    "\n",
    "\n",
    "def format_cell(mean, std, normalized):\n",
    "    return f\"{mean:.3f}±{std:.3f}\" if normalized else f\"{mean:.1f}±{std:.1f}\"\n",
    "\n",
    "\n",
    "def aggregate_cm_mean_std(df_conf_all, normalize=None):\n",
    "    out = {}\n",
    "    for model, g in df_conf_all.groupby(\"model\"):\n",
    "        mats = []\n",
    "        for _, r in g.iterrows():\n",
    "            cm = np.array([[r['tn'], r['fp']], [r['fn'], r['tp']]], dtype=float)\n",
    "            mats.append(normalize_cm(cm, normalize))\n",
    "        if not mats:\n",
    "            continue\n",
    "        arr = np.stack(mats, axis=0)\n",
    "        out[model] = {\n",
    "            'mean': arr.mean(axis=0),\n",
    "            'std':  arr.std(axis=0, ddof=1) if arr.shape[0] > 1 else np.zeros((2,2)),\n",
    "            'n':    int(arr.shape[0])\n",
    "        }\n",
    "    return out\n",
    "\n",
    "\n",
    "def plot_confusion_mean_std(agg, normalize=None, classes=None, out_path=None, cmap=\"Blues\"):\n",
    "    \"\"\"\n",
    "    Annotated confusion matrices for all models.\n",
    "    Updated for Pediatric IIH vs Control labels.\n",
    "    \"\"\"\n",
    "    models = list(agg.keys())\n",
    "    if not models:\n",
    "        raise ValueError(\"No models to plot.\")\n",
    "    n = len(models)\n",
    "    n_cols = min(3, n)\n",
    "    n_rows = int(np.ceil(n / n_cols))\n",
    "    figsize = (5*n_cols, 4.5*n_rows)\n",
    "    normalized = (normalize is not None)\n",
    "\n",
    "    # Default labels for Pediatric IIH work\n",
    "    if classes is None:\n",
    "        classes = [\"Control\", \"Pediatric IIH\"]\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "    if n_rows == 1 and n_cols == 1:\n",
    "        axes = np.array([[axes]])\n",
    "    elif n_rows == 1:\n",
    "        axes = np.array([axes])\n",
    "    elif n_cols == 1:\n",
    "        axes = np.array([[ax] for ax in axes])\n",
    "\n",
    "    vmin, vmax = (0.0, 1.0) if normalized else (None, None)\n",
    "\n",
    "    idx = 0\n",
    "    for r in range(n_rows):\n",
    "        for c in range(n_cols):\n",
    "            ax = axes[r, c]\n",
    "            if idx >= n:\n",
    "                ax.axis(\"off\"); continue\n",
    "            model = models[idx]\n",
    "            M, S, n_runs = agg[model]['mean'], agg[model]['std'], agg[model]['n']\n",
    "\n",
    "            ax.imshow(M, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "\n",
    "            for i in range(2):\n",
    "                for j in range(2):\n",
    "                    ax.text(j, i, format_cell(M[i, j], S[i, j], normalized),\n",
    "                            color=\"black\", ha=\"center\", va=\"center\", fontsize=14)\n",
    "\n",
    "            ax.set_xticks([0, 1]); ax.set_yticks([0, 1])\n",
    "            ax.set_xticklabels([f\"Pred {classes[0]}\", f\"Pred {classes[1]}\"])\n",
    "            ax.set_yticklabels([f\"True {classes[0]}\", f\"True {classes[1]}\"])\n",
    "\n",
    "            ax.set_title(f\"{model} — mean±std (n={n_runs})\")\n",
    "            ax.set_xticks(np.arange(-.5, 2, 1), minor=True)\n",
    "            ax.set_yticks(np.arange(-.5, 2, 1), minor=True)\n",
    "            ax.grid(which='minor', color='w', linestyle='-', linewidth=1)\n",
    "            ax.tick_params(which='minor', bottom=False, left=False)\n",
    "            idx += 1\n",
    "\n",
    "    # Colorbar\n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "    fig.colorbar(axes[0,0].images[0], cax=cbar_ax)\n",
    "\n",
    "    supt = \"Confusion Matrices — Pediatric IIH vs Controls (mean ± std)\"\n",
    "    if normalize == \"row\": supt += \" (row-normalized)\"\n",
    "    elif normalize == \"all\": supt += \" (global-normalized)\"\n",
    "\n",
    "    fig.suptitle(supt, y=0.98, fontsize=14)\n",
    "    fig.tight_layout(rect=[0, 0, 0.90, 0.97])\n",
    "\n",
    "    if out_path is not None:\n",
    "        fig.savefig(out_path, dpi=200)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc43e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===============================================================\n",
    "# # 5) Optuna Objective & Param Sanitization\n",
    "# # ===============================================================\n",
    "# def objective(trial, model_name, X_train, y_train, X_val, y_val, preprocessor_fixed, seed=None):\n",
    "#     \"\"\"\n",
    "#     Optuna objective for Pediatric IIH classification.\n",
    "#     The positive class is Pediatric IIH, negative class is Control.\n",
    "#     Builds model, applies SelectKBest, fits on TRAIN, and returns\n",
    "#     the selected validation objective (AP / AUC / F1).\n",
    "#     \"\"\"\n",
    "\n",
    "#     rng_seed = (RANDOM_BASE if seed is None else int(seed))\n",
    "\n",
    "#     # ---------------- Classifier space ----------------\n",
    "#     if model_name == \"Random Forest\":\n",
    "#         params = {\n",
    "#             'n_estimators':       trial.suggest_int('n_estimators', 200, 1000, step=100),\n",
    "#             'max_depth':          trial.suggest_int('max_depth', 2, 30),\n",
    "#             'min_samples_split':  trial.suggest_int('min_samples_split', 2, 20),\n",
    "#             'min_samples_leaf':   trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "#             'max_features':       trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "#             'criterion':          trial.suggest_categorical('criterion', ['gini', 'entropy', 'log_loss']),\n",
    "#             'bootstrap':          trial.suggest_categorical('bootstrap', [True, False]),\n",
    "#             'class_weight':       'balanced',\n",
    "#             'n_jobs':            -1,\n",
    "#             'random_state':       rng_seed\n",
    "#         }\n",
    "#         clf = RandomForestClassifier(**params)\n",
    "\n",
    "#     elif model_name == \"SVM\":\n",
    "#         kernel = trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly'])\n",
    "#         params = {\n",
    "#             'C':            trial.suggest_float('C', 1e-3, 1e3, log=True),\n",
    "#             'kernel':       kernel,\n",
    "#             'class_weight': 'balanced',\n",
    "#             'probability':  True,\n",
    "#             'random_state': rng_seed,\n",
    "#         }\n",
    "#         if kernel in ('rbf', 'poly'):\n",
    "#             gm = trial.suggest_categorical('gamma_mode', ['scale', 'auto', 'float'])\n",
    "#             params['gamma'] = trial.suggest_float('gamma', 1e-4, 1.0, log=True) if gm == 'float' else gm\n",
    "#         if kernel == 'poly':\n",
    "#             params['degree'] = trial.suggest_int('degree', 2, 5)\n",
    "#         clf = SVC(**params)\n",
    "\n",
    "#     elif model_name == \"MLP\":\n",
    "#         solver = trial.suggest_categorical('solver', ['adam', 'lbfgs'])\n",
    "#         params = {\n",
    "#             'hidden_layer_sizes': trial.suggest_categorical(\n",
    "#                 'hidden_layer_sizes', [(20,10,5), (15,15,5), (20,15,5), (20,10)]\n",
    "#             ),\n",
    "#             'activation':   trial.suggest_categorical('activation', ['relu', 'tanh', 'logistic']),\n",
    "#             'alpha':        trial.suggest_float('alpha', 1e-5, 1e-1, log=True),\n",
    "#             'solver':       solver,\n",
    "#             'max_iter':     trial.suggest_int('max_iter', 400, 1200, step=100),\n",
    "#             'random_state': rng_seed,\n",
    "#         }\n",
    "#         if solver == 'adam':\n",
    "#             params.update({\n",
    "#                 'learning_rate':      trial.suggest_categorical('learning_rate', ['constant', 'adaptive']),\n",
    "#                 'learning_rate_init': trial.suggest_float('learning_rate_init', 1e-5, 1e-3, log=True),\n",
    "#                 'beta_1':             trial.suggest_float('beta_1', 0.7, 0.99),\n",
    "#                 'beta_2':             trial.suggest_float('beta_2', 0.9, 0.999),\n",
    "#                 'early_stopping':     trial.suggest_categorical('early_stopping', [True, False]),\n",
    "#                 'batch_size':         trial.suggest_categorical('batch_size', [16, 32, 64]),\n",
    "#             })\n",
    "#         clf = MLPClassifier(**params)\n",
    "\n",
    "#     elif model_name == \"XGBoost\":\n",
    "#         # positive class = Pediatric IIH\n",
    "#         pos_n = int(np.sum(y_train == pos_label))\n",
    "#         neg_n = int(np.sum(y_train != pos_label))\n",
    "#         spw_base = (neg_n / max(1, pos_n)) if pos_n > 0 else 1.0\n",
    "\n",
    "#         params = {\n",
    "#             'n_estimators':      trial.suggest_int('n_estimators', 200, 1000, step=100),\n",
    "#             'max_depth':         trial.suggest_int('max_depth', 2, 10),\n",
    "#             'learning_rate':     trial.suggest_float('learning_rate', 1e-3, 3e-1, log=True),\n",
    "#             'subsample':         trial.suggest_float('subsample', 0.5, 1.0),\n",
    "#             'colsample_bytree':  trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "#             'min_child_weight':  trial.suggest_float('min_child_weight', 1e-1, 10.0, log=True),\n",
    "#             'gamma':             trial.suggest_float('gamma', 0.0, 5.0),\n",
    "#             'reg_alpha':         trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
    "#             'reg_lambda':        trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),\n",
    "#             'scale_pos_weight':  trial.suggest_float('scale_pos_weight', spw_base*0.5, spw_base*2.0, log=True),\n",
    "#             'n_jobs':           -1,\n",
    "#             'random_state':      rng_seed,\n",
    "#             'eval_metric':       'logloss',\n",
    "#             'tree_method':       'hist',\n",
    "#         }\n",
    "#         clf = XGBClassifier(**params)\n",
    "\n",
    "#     elif model_name == \"KNN\":\n",
    "#         params = {\n",
    "#             \"n_neighbors\": trial.suggest_int(\"n_neighbors\", 1, 30),\n",
    "#             \"weights\":     trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"]),\n",
    "#             \"p\":           trial.suggest_int(\"p\", 1, 2),\n",
    "#             \"leaf_size\":   trial.suggest_int(\"leaf_size\", 10, 60),\n",
    "#             \"metric\":      trial.suggest_categorical(\"metric\", [\"minkowski\", \"euclidean\", \"manhattan\"]),\n",
    "#         }\n",
    "#         clf = KNeighborsClassifier(**params)\n",
    "\n",
    "#     elif model_name == \"Bagging\":\n",
    "#         base_choice = trial.suggest_categorical(\"base_estimator\", [\"tree\", \"knn\"])\n",
    "#         if base_choice == \"tree\":\n",
    "#             base_est = DecisionTreeClassifier(\n",
    "#                 max_depth=trial.suggest_int(\"max_depth\", 1, 15),\n",
    "#                 min_samples_split=trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "#                 min_samples_leaf=trial.suggest_int(\"min_samples_leaf\", 1, 5),\n",
    "#                 class_weight='balanced',\n",
    "#                 random_state=rng_seed\n",
    "#             )\n",
    "#         else:\n",
    "#             base_est = KNeighborsClassifier(\n",
    "#                 n_neighbors=trial.suggest_int(\"n_neighbors\", 3, 15),\n",
    "#                 weights=trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"])\n",
    "#             )\n",
    "#         params = {\n",
    "#             \"n_estimators\":       trial.suggest_int(\"n_estimators\", 10, 200, step=10),\n",
    "#             \"max_samples\":        trial.suggest_float(\"max_samples\", 0.5, 1.0),\n",
    "#             \"max_features\":       trial.suggest_float(\"max_features\", 0.5, 1.0),\n",
    "#             \"bootstrap\":          trial.suggest_categorical(\"bootstrap\", [True, False]),\n",
    "#             \"bootstrap_features\": trial.suggest_categorical(\"bootstrap_features\", [False, True]),\n",
    "#             \"n_jobs\":            -1,\n",
    "#             \"random_state\":       rng_seed,\n",
    "#         }\n",
    "#         clf = BaggingClassifier(estimator=base_est, **params)\n",
    "\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unknown model: {model_name}\")\n",
    "\n",
    "#     # ---------------- Feature selector ----------------\n",
    "#     n_out_after_prep = X_train.shape[1]\n",
    "#     k_min_eff = int(max(1, min(K_MIN, n_out_after_prep)))\n",
    "#     k_max_eff = int(min(K_MAX, n_out_after_prep))\n",
    "#     if k_max_eff < k_min_eff:\n",
    "#         k_max_eff = k_min_eff\n",
    "#     k_best = trial.suggest_int('k_best', k_min_eff, k_max_eff)\n",
    "\n",
    "#     selector = SelectKBest(score_func=f_classif, k=k_best)\n",
    "\n",
    "#     # ---------------- Full pipeline ----------------\n",
    "#     pipe = Pipeline([\n",
    "#         ('prep', preprocessor_fixed),\n",
    "#         ('feat', selector),\n",
    "#         ('clf', clf)\n",
    "#     ])\n",
    "\n",
    "#     pipe.fit(X_train, y_train)\n",
    "\n",
    "#     # Validation predictions\n",
    "#     y_pred = pipe.predict(X_val)\n",
    "#     f1_val = f1_score(y_val, y_pred, average=\"binary\", pos_label=pos_label)\n",
    "\n",
    "#     # Need probabilities for AP/AUC\n",
    "#     proba_val = pipe.predict_proba(X_val)[:, \n",
    "#         int(np.where(pipe.named_steps[\"clf\"].classes_ == pos_label)[0][0])\n",
    "#     ]\n",
    "#     ap_val  = average_precision_score(y_val, proba_val)\n",
    "#     auc_val = roc_auc_score(y_val, proba_val)\n",
    "\n",
    "#     # Log for Optuna dashboard\n",
    "#     trial.set_user_attr(\"val_f1\",  float(f1_val))\n",
    "#     trial.set_user_attr(\"val_ap\",  float(ap_val))\n",
    "#     trial.set_user_attr(\"val_auc\", float(auc_val))\n",
    "\n",
    "#     # Return chosen metric\n",
    "#     if OPTIMIZE_FOR.lower() == \"ap\":\n",
    "#         return ap_val\n",
    "#     elif OPTIMIZE_FOR.lower() == \"auc\":\n",
    "#         return auc_val\n",
    "#     else:\n",
    "#         return f1_val\n",
    "\n",
    "\n",
    "# def sanitize_params_for_final(model_name, best_params_all):\n",
    "#     \"\"\"\n",
    "#     Remove pipeline-only params from Optuna (k_best, gamma_mode...).\n",
    "#     \"\"\"\n",
    "#     p = best_params_all.copy()\n",
    "#     for k in [\"k_best\", \"k_features\"]:\n",
    "#         p.pop(k, None)\n",
    "\n",
    "#     if model_name == \"SVM\":\n",
    "#         gm = p.pop(\"gamma_mode\", None)\n",
    "#         if gm in (\"scale\", \"auto\"):\n",
    "#             p[\"gamma\"] = gm\n",
    "\n",
    "#     if model_name == \"Bagging\":\n",
    "#         base_choice = p.pop(\"base_estimator\", None)\n",
    "#         allowed = {\"n_estimators\", \"max_samples\", \"max_features\",\n",
    "#                    \"bootstrap\", \"bootstrap_features\", \"n_jobs\", \"random_state\"}\n",
    "#         p = {k: v for k, v in p.items() if k in allowed}\n",
    "#         return p, base_choice\n",
    "\n",
    "#     return p, None\n",
    "\n",
    "\n",
    "# def get_selected_feature_info(fitted_pipe: Pipeline, X_ref: pd.DataFrame):\n",
    "#     \"\"\"\n",
    "#     Returns:\n",
    "#         selected_mask,\n",
    "#         selected_raw_names,\n",
    "#         selected_clean_names,\n",
    "#         all_base_names\n",
    "#     \"\"\"\n",
    "#     pre = fitted_pipe.named_steps.get('prep', None)\n",
    "#     feat = fitted_pipe.named_steps.get('feat', None)\n",
    "\n",
    "#     try:\n",
    "#         base_names = list(pre.get_feature_names_out(input_features=X_ref.columns))\n",
    "#     except Exception:\n",
    "#         try:\n",
    "#             base_names = list(pre.get_feature_names_out())\n",
    "#         except Exception:\n",
    "#             base_names = list(X_ref.columns)\n",
    "\n",
    "#     mask = None\n",
    "#     if feat is not None and hasattr(feat, \"get_support\"):\n",
    "#         mask = feat.get_support()\n",
    "#         sel_names = list(np.array(base_names)[mask])\n",
    "#     else:\n",
    "#         sel_names = base_names\n",
    "\n",
    "#     sel_names_clean = clean_names(sel_names)\n",
    "#     return mask, sel_names, sel_names_clean, base_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3117836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# 5) Optuna Objective & Param Sanitization  (FULL SAFE VERSION)\n",
    "# ===============================================================\n",
    "\n",
    "def objective(trial, model_name, X_train, y_train, X_val, y_val, preprocessor_fixed, seed=None):\n",
    "    \"\"\"\n",
    "    SAFE Optuna objective for Pediatric IIH classification.\n",
    "    Includes:\n",
    "      - SAFE KNN\n",
    "      - SAFE Bagging KNN\n",
    "      - Expanded HP search space\n",
    "      - Feature selection K ∈ [3,15]\n",
    "    \"\"\"\n",
    "\n",
    "    rng_seed = (RANDOM_BASE if seed is None else int(seed))\n",
    "\n",
    "    # ============================================================\n",
    "    # RANDOM FOREST\n",
    "    # ============================================================\n",
    "    if model_name == \"Random Forest\":\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 200, 2000, step=200),\n",
    "            'max_depth': trial.suggest_int('max_depth', 2, 80),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 50),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "            'max_features': trial.suggest_categorical('max_features',\n",
    "                                                      ['sqrt','log2',None,0.3,0.5,0.7]),\n",
    "            'criterion': trial.suggest_categorical('criterion',\n",
    "                                                   ['gini','entropy','log_loss']),\n",
    "            'bootstrap': trial.suggest_categorical('bootstrap',[True,False]),\n",
    "            'class_weight': 'balanced',\n",
    "            'n_jobs': -1,\n",
    "            'random_state': rng_seed\n",
    "        }\n",
    "        clf = RandomForestClassifier(**params)\n",
    "\n",
    "    # ============================================================\n",
    "    # SVM\n",
    "    # ============================================================\n",
    "    elif model_name == \"SVM\":\n",
    "        kernel = trial.suggest_categorical('kernel',['linear','rbf','poly','sigmoid'])\n",
    "        params = {\n",
    "            'C': trial.suggest_float('C',1e-4,1e4,log=True),\n",
    "            'kernel': kernel,\n",
    "            'class_weight': 'balanced',\n",
    "            'probability': True,\n",
    "            'random_state': rng_seed,\n",
    "        }\n",
    "        if kernel in ['rbf','poly','sigmoid']:\n",
    "            gm = trial.suggest_categorical('gamma_mode',['scale','auto','float'])\n",
    "            params['gamma'] = trial.suggest_float('gamma',1e-6,10.0,log=True) if gm==\"float\" else gm\n",
    "        if kernel == 'poly':\n",
    "            params['degree'] = trial.suggest_int('degree',2,7)\n",
    "            params['coef0'] = trial.suggest_float('coef0',-5,5)\n",
    "        if kernel == 'sigmoid':\n",
    "            params['coef0'] = trial.suggest_float('coef0',-5,5)\n",
    "\n",
    "        clf = SVC(**params)\n",
    "\n",
    "    # ============================================================\n",
    "    # MLP\n",
    "    # ============================================================\n",
    "    elif model_name == \"MLP\":\n",
    "        params = {\n",
    "            \"hidden_layer_sizes\": trial.suggest_categorical(\n",
    "                \"hidden_layer_sizes\",\n",
    "                [(50,), (100,), (150,),\n",
    "                 (50,25), (100,50), (150,75),\n",
    "                 (100,100), (150,100),\n",
    "                 (200,100,50), (300,150,75)]\n",
    "            ),\n",
    "            \"activation\": trial.suggest_categorical(\"activation\",[\"relu\",\"tanh\",\"logistic\"]),\n",
    "            \"alpha\": trial.suggest_float(\"alpha\",1e-6,1e-1,log=True),\n",
    "            \"max_iter\": trial.suggest_int(\"max_iter\",300,3000,step=300),\n",
    "            \"solver\": trial.suggest_categorical(\"solver\",[\"adam\",\"lbfgs\"]),\n",
    "            \"random_state\": rng_seed\n",
    "        }\n",
    "\n",
    "        if params[\"solver\"] == \"adam\":\n",
    "            params.update({\n",
    "                \"learning_rate\": trial.suggest_categorical(\"learning_rate\",[\"constant\",\"adaptive\"]),\n",
    "                \"learning_rate_init\": trial.suggest_float(\"learning_rate_init\",1e-6,5e-3,log=True),\n",
    "                \"beta_1\": trial.suggest_float(\"beta_1\",0.5,0.99),\n",
    "                \"beta_2\": trial.suggest_float(\"beta_2\",0.8,0.999),\n",
    "                \"batch_size\": trial.suggest_categorical(\"batch_size\",[16,32,64,128]),\n",
    "                \"early_stopping\": trial.suggest_categorical(\"early_stopping\",[True,False])\n",
    "            })\n",
    "\n",
    "        clf = MLPClassifier(**params)\n",
    "\n",
    "    # ============================================================\n",
    "    # XGBOOST\n",
    "    # ============================================================\n",
    "    elif model_name == \"XGBoost\":\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\",300,3000,step=300),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\",2,20),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\",1e-4,0.3,log=True),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\",0.3,1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\",0.3,1.0),\n",
    "            \"min_child_weight\": trial.suggest_float(\"min_child_weight\",1e-3,50,log=True),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\",0.0,10.0),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\",1e-8,10.0,log=True),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\",1e-3,10.0,log=True),\n",
    "            \"scale_pos_weight\": trial.suggest_float(\"scale_pos_weight\",0.5,3.0),\n",
    "            \"eval_metric\": \"logloss\",\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"n_jobs\": -1,\n",
    "            \"random_state\": rng_seed\n",
    "        }\n",
    "        clf = XGBClassifier(**params)\n",
    "\n",
    "    # ============================================================\n",
    "    # SAFE KNN\n",
    "    # ============================================================\n",
    "    elif model_name == \"KNN\":\n",
    "\n",
    "        safe_k = min(15, len(X_train) - 1)\n",
    "\n",
    "        params = {\n",
    "            \"n_neighbors\": trial.suggest_int(\"n_neighbors\", 1, safe_k),\n",
    "            \"weights\": trial.suggest_categorical(\"weights\",[\"uniform\",\"distance\"]),\n",
    "            \"p\": trial.suggest_int(\"p\",1,5),\n",
    "            \"leaf_size\": trial.suggest_int(\"leaf_size\",10,100),\n",
    "            \"metric\": trial.suggest_categorical(\"metric\",\n",
    "                                                [\"minkowski\",\"euclidean\",\"manhattan\",\"chebyshev\"])\n",
    "        }\n",
    "        clf = KNeighborsClassifier(**params)\n",
    "\n",
    "    # ============================================================\n",
    "    # SAFE BAGGING (SAFE TREE / SAFE KNN)\n",
    "    # ============================================================\n",
    "    elif model_name == \"Bagging\":\n",
    "\n",
    "        base_choice = trial.suggest_categorical(\"base_estimator\",[\"tree\",\"knn\"])\n",
    "\n",
    "        if base_choice == \"tree\":\n",
    "            base_est = DecisionTreeClassifier(\n",
    "                max_depth=trial.suggest_int(\"max_depth\",1,50),\n",
    "                min_samples_split=trial.suggest_int(\"min_samples_split\",2,50),\n",
    "                min_samples_leaf=trial.suggest_int(\"min_samples_leaf\",1,20),\n",
    "                class_weight=\"balanced\",\n",
    "                random_state=rng_seed\n",
    "            )\n",
    "        else:\n",
    "            safe_k_bag = min(10, len(X_train) - 1)\n",
    "            base_est = KNeighborsClassifier(\n",
    "                n_neighbors=trial.suggest_int(\"n_neighbors\",1,safe_k_bag),\n",
    "                weights=trial.suggest_categorical(\"weights\",[\"uniform\",\"distance\"])\n",
    "            )\n",
    "\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\",50,500,step=50),\n",
    "            \"max_samples\": trial.suggest_float(\"max_samples\",0.3,1.0),\n",
    "            \"max_features\": trial.suggest_float(\"max_features\",0.3,1.0),\n",
    "            \"bootstrap\": trial.suggest_categorical(\"bootstrap\",[True,False]),\n",
    "            \"bootstrap_features\": trial.suggest_categorical(\"bootstrap_features\",[False,True]),\n",
    "            \"n_jobs\": -1,\n",
    "            \"random_state\": rng_seed\n",
    "        }\n",
    "\n",
    "        clf = BaggingClassifier(estimator=base_est, **params)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "\n",
    "    # ============================================================\n",
    "    # FEATURE SELECTION  (K = 3–15)\n",
    "    # ============================================================\n",
    "    n_feat = X_train.shape[1]\n",
    "    k_best = trial.suggest_int(\"k_best\", 3, min(15, n_feat))\n",
    "\n",
    "    selector = SelectKBest(score_func=f_classif, k=k_best)\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        (\"prep\", preprocessor_fixed),\n",
    "        (\"feat\", selector),\n",
    "        (\"clf\",  clf)\n",
    "    ])\n",
    "\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # ============================================================\n",
    "    # VALIDATION METRICS\n",
    "    # ============================================================\n",
    "    y_pred = pipe.predict(X_val)\n",
    "    f1_val = f1_score(y_val, y_pred, pos_label=pos_label)\n",
    "\n",
    "    proba_val = pipe.predict_proba(X_val)[:, int(np.where(pipe.named_steps[\"clf\"].classes_ == pos_label)[0][0])]\n",
    "    ap_val = average_precision_score(y_val, proba_val)\n",
    "    auc_val = roc_auc_score(y_val, proba_val)\n",
    "\n",
    "    trial.set_user_attr(\"val_f1\", float(f1_val))\n",
    "    trial.set_user_attr(\"val_ap\", float(ap_val))\n",
    "    trial.set_user_attr(\"val_auc\", float(auc_val))\n",
    "\n",
    "    if OPTIMIZE_FOR==\"ap\":\n",
    "        return ap_val\n",
    "    if OPTIMIZE_FOR==\"auc\":\n",
    "        return auc_val\n",
    "    return f1_val\n",
    "\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# PARAM SANITIZATION — FINAL SAFE CLEANUP\n",
    "# ===============================================================\n",
    "\n",
    "def sanitize_params_for_final(model_name, best_params_all):\n",
    "    \"\"\"\n",
    "    Final cleanup BEFORE model instantiation in block 6.\n",
    "    Ensures:\n",
    "      - SAFE KNN everywhere\n",
    "      - SAFE BAGGING KNN\n",
    "      - removes k_best, gamma_mode, base_estimator noise\n",
    "    \"\"\"\n",
    "\n",
    "    p = best_params_all.copy()\n",
    "\n",
    "    # Remove FS leftover keys\n",
    "    for k in [\"k_best\", \"k_features\"]:\n",
    "        p.pop(k, None)\n",
    "\n",
    "    # ---------------- SVM CLEANUP ----------------\n",
    "    if model_name == \"SVM\":\n",
    "        gm = p.pop(\"gamma_mode\", None)\n",
    "        if gm in [\"scale\", \"auto\"]:\n",
    "            p[\"gamma\"] = gm\n",
    "\n",
    "    # ---------------- SAFE KNN CLEANUP ----------------\n",
    "    if model_name == \"KNN\":\n",
    "        cleaned = {}\n",
    "        for k, v in p.items():\n",
    "            if k == \"n_neighbors\":\n",
    "                cleaned[k] = min(v, 15)\n",
    "            else:\n",
    "                cleaned[k] = v\n",
    "        return cleaned, None\n",
    "\n",
    "    # ---------------- SAFE BAGGING ----------------\n",
    "    if model_name == \"Bagging\":\n",
    "        base_choice = p.pop(\"base_estimator\", None)\n",
    "        cleaned = {\n",
    "            k:v for k,v in p.items()\n",
    "            if k in [\n",
    "                \"n_estimators\",\"max_samples\",\"max_features\",\n",
    "                \"bootstrap\",\"bootstrap_features\",\"n_jobs\",\"random_state\"\n",
    "            ]\n",
    "        }\n",
    "        return cleaned, base_choice\n",
    "\n",
    "    return p, None\n",
    "\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# FEATURE NAME EXTRACTION (Correct & Complete)\n",
    "# ===============================================================\n",
    "\n",
    "def get_selected_feature_info(fitted_pipe: Pipeline, X_ref: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Extract selected feature names safely.\n",
    "    Preprocessing keeps column count → X_ref.columns safe.\n",
    "    \"\"\"\n",
    "\n",
    "    feat = fitted_pipe.named_steps.get(\"feat\", None)\n",
    "\n",
    "    base_names = list(X_ref.columns)\n",
    "\n",
    "    if feat is not None and hasattr(feat, \"get_support\"):\n",
    "        mask = feat.get_support()\n",
    "        sel_names = list(np.array(base_names)[mask])\n",
    "    else:\n",
    "        mask = None\n",
    "        sel_names = base_names\n",
    "\n",
    "    sel_names_clean = clean_names(sel_names)\n",
    "\n",
    "    return mask, sel_names, sel_names_clean, base_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2347e765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===============================================================\n",
    "# # 6) Main Loop: Splits + Optuna + Test Evaluation\n",
    "# # ===============================================================\n",
    "# model_names = [\"Random Forest\", \"SVM\", \"MLP\", \"XGBoost\", \"KNN\", \"Bagging\"]\n",
    "\n",
    "# best_hyperparameters    = {m: [] for m in model_names}\n",
    "# selected_features_store = {m: {} for m in model_names}\n",
    "\n",
    "# test_confusions = {m: [] for m in model_names}\n",
    "# test_roc_curves = {m: [] for m in model_names}  # (fpr, tpr, auc)\n",
    "# test_pr_curves  = {m: [] for m in model_names}  # (recall, precision, ap)\n",
    "\n",
    "# test_auc_list   = {m: [] for m in model_names}\n",
    "# test_ap_list    = {m: [] for m in model_names}\n",
    "# test_f1_list    = {m: [] for m in model_names}\n",
    "\n",
    "# final_pipes     = {m: {} for m in model_names}\n",
    "# saved_model_paths = {m: [] for m in model_names}\n",
    "# split_store     = {}\n",
    "# ap_records      = []\n",
    "\n",
    "# for rep in range(N_REPS):\n",
    "#     # Seeds for reproducibility across reps\n",
    "#     seed_outer = int(RANDOM_BASE + 10007*rep + 13)\n",
    "#     seed_inner = int(RANDOM_BASE + 20011*rep + 97)\n",
    "#     seed_model = int(RANDOM_BASE + 30029*rep + 211)\n",
    "#     seed_tpe   = int(RANDOM_BASE + 40039*rep + 509)\n",
    "\n",
    "#     # ======================================================\n",
    "#     # OUTER SPLIT (TEST 20%) — stratified by Control / IIH\n",
    "#     # ======================================================\n",
    "#     outer = StratifiedShuffleSplit(n_splits=1, test_size=TEST_FRACTION, random_state=seed_outer)\n",
    "#     (trainval_idx, test_idx), = outer.split(X, y)\n",
    "#     X_trainval, y_trainval = X.iloc[trainval_idx], y[trainval_idx]\n",
    "#     X_test,     y_test     = X.iloc[test_idx],     y[test_idx]\n",
    "#     split_store[rep] = {\"trainval_idx\": trainval_idx.tolist(), \"test_idx\": test_idx.tolist()}\n",
    "\n",
    "#     # ======================================================\n",
    "#     # INNER SPLIT (TRAIN/VAL)\n",
    "#     # ======================================================\n",
    "#     inner = StratifiedShuffleSplit(\n",
    "#         n_splits=1,\n",
    "#         train_size=train_in_trainval,\n",
    "#         test_size=val_in_trainval,\n",
    "#         random_state=seed_inner\n",
    "#     )\n",
    "#     (tr_idx, val_idx), = inner.split(X_trainval, y_trainval)\n",
    "#     X_train, y_train = X_trainval.iloc[tr_idx], y_trainval[tr_idx]\n",
    "#     X_val,   y_val   = X_trainval.iloc[val_idx], y_trainval[val_idx]\n",
    "\n",
    "#     PREPROC = build_preprocessor_all_robust(X_train)\n",
    "\n",
    "#     # ======================================================\n",
    "#     # MODEL OPTIMIZATION LOOP\n",
    "#     # ======================================================\n",
    "#     for model_name in model_names:\n",
    "\n",
    "#         # ---- Optuna study ----\n",
    "#         study = optuna.create_study(\n",
    "#             direction=\"maximize\",\n",
    "#             sampler=optuna.samplers.TPESampler(seed=seed_tpe),\n",
    "#             pruner=optuna.pruners.MedianPruner(n_warmup_steps=10),\n",
    "#         )\n",
    "#         study.optimize(\n",
    "#             lambda tr: objective(\n",
    "#                 tr, model_name,\n",
    "#                 X_train, y_train,\n",
    "#                 X_val, y_val,\n",
    "#                 preprocessor_fixed=PREPROC,\n",
    "#                 seed=seed_model\n",
    "#             ),\n",
    "#             n_trials=N_TRIALS_BY_MODEL.get(model_name, 150),\n",
    "#             n_jobs=1,\n",
    "#         )\n",
    "\n",
    "#         best_params_all = study.best_params.copy()\n",
    "#         bt = study.best_trial\n",
    "\n",
    "#         best_val_f1  = float(bt.user_attrs.get(\"val_f1\",  np.nan))\n",
    "#         best_val_ap  = float(bt.user_attrs.get(\"val_ap\",  np.nan))\n",
    "#         best_val_auc = float(bt.user_attrs.get(\"val_auc\", np.nan))\n",
    "\n",
    "#         # Record best hyperparameters\n",
    "#         best_hyperparameters[model_name].append({\n",
    "#             \"rep\": rep,\n",
    "#             \"best_params\": best_params_all.copy(),\n",
    "#             \"val_f1\":  best_val_f1,\n",
    "#             \"val_ap\":  best_val_ap,\n",
    "#             \"val_auc\": best_val_auc,\n",
    "#             \"optimize_for\": OPTIMIZE_FOR,\n",
    "#             \"clinical_context\": \"Binary classification: Control (0) vs Pediatric IIH (1)\"\n",
    "#         })\n",
    "\n",
    "#         clean_params, base_choice = sanitize_params_for_final(model_name, best_params_all)\n",
    "#         k_best = int(best_params_all.get(\"k_best\", min(K_MAX, X_train.shape[1])))\n",
    "\n",
    "#         # ======================================================\n",
    "#         # FINAL MODEL TRAINING ON TRAIN+VAL\n",
    "#         # ======================================================\n",
    "#         if model_name == \"Random Forest\":\n",
    "#             clf_best = RandomForestClassifier(\n",
    "#                 **clean_params,\n",
    "#                 n_jobs=-1,\n",
    "#                 class_weight='balanced',\n",
    "#                 random_state=seed_model\n",
    "#             )\n",
    "#         elif model_name == \"SVM\":\n",
    "#             clf_best = SVC(\n",
    "#                 probability=True,\n",
    "#                 **clean_params,\n",
    "#                 class_weight='balanced',\n",
    "#                 random_state=seed_model\n",
    "#             )\n",
    "#         elif model_name == \"MLP\":\n",
    "#             clf_best = MLPClassifier(**clean_params, random_state=seed_model)\n",
    "#         elif model_name == \"XGBoost\":\n",
    "#             clf_best = XGBClassifier(\n",
    "#                 eval_metric=\"logloss\",\n",
    "#                 n_jobs=-1,\n",
    "#                 random_state=seed_model,\n",
    "#                 **clean_params\n",
    "#             )\n",
    "#         elif model_name == \"KNN\":\n",
    "#             clf_best = KNeighborsClassifier(**clean_params)\n",
    "#         elif model_name == \"Bagging\":\n",
    "#             if base_choice is None:\n",
    "#                 base_choice = best_params_all.get(\"base_estimator\", \"tree\")\n",
    "#             if base_choice == \"tree\":\n",
    "#                 base_est = DecisionTreeClassifier(\n",
    "#                     max_depth=best_params_all.get(\"max_depth\", None),\n",
    "#                     min_samples_split=best_params_all.get(\"min_samples_split\", 2),\n",
    "#                     min_samples_leaf=best_params_all.get(\"min_samples_leaf\", 1),\n",
    "#                     class_weight='balanced',\n",
    "#                     random_state=seed_model,\n",
    "#                 )\n",
    "#             else:\n",
    "#                 base_est = KNeighborsClassifier(\n",
    "#                     n_neighbors=best_params_all.get(\"n_neighbors\", 5),\n",
    "#                     weights=best_params_all.get(\"weights\", \"uniform\")\n",
    "#                 )\n",
    "#             bag_params = {k: v for k, v in best_params_all.items() if k in {\n",
    "#                 \"n_estimators\", \"max_samples\", \"max_features\",\n",
    "#                 \"bootstrap\", \"bootstrap_features\", \"n_jobs\", \"random_state\"\n",
    "#             }}\n",
    "#             clf_best = BaggingClassifier(estimator=base_est, random_state=seed_model, **bag_params)\n",
    "#         else:\n",
    "#             raise ValueError(f\"Unknown model: {model_name}\")\n",
    "\n",
    "#         selector_final = SelectKBest(score_func=f_classif, k=k_best)\n",
    "#         final_pipe = Pipeline([\n",
    "#             (\"prep\", PREPROC),\n",
    "#             (\"feat\", selector_final),\n",
    "#             (\"clf\",  clf_best)\n",
    "#         ])\n",
    "#         final_pipe.fit(X_trainval, y_trainval)\n",
    "\n",
    "#         # ======================================================\n",
    "#         # SELECTED FEATURES STORAGE\n",
    "#         # ======================================================\n",
    "#         mask, sel_names, sel_names_clean, base_names = get_selected_feature_info(final_pipe, X)\n",
    "#         selected_features_store[model_name][rep] = {\n",
    "#             \"k_best\": k_best,\n",
    "#             \"mask_after_prep\": None if mask is None else mask.astype(bool).tolist(),\n",
    "#             \"selected_feature_names_after_prep\": sel_names,\n",
    "#             \"selected_feature_names_clean\": sel_names_clean,\n",
    "#             \"base_feature_names_after_prep\": base_names,\n",
    "#             \"f_scores_all\": getattr(selector_final, \"scores_\", None).tolist() \n",
    "#                                 if getattr(selector_final, \"scores_\", None) is not None else None,\n",
    "#             \"p_values_all\": getattr(selector_final, \"pvalues_\", None).tolist()\n",
    "#                                 if getattr(selector_final, \"pvalues_\", None) is not None else None\n",
    "#         }\n",
    "\n",
    "#         # ======================================================\n",
    "#         # TEST EVALUATION (20% unseen)\n",
    "#         # ======================================================\n",
    "#         pos_idx = int(np.where(final_pipe.named_steps[\"clf\"].classes_ == pos_label)[0][0])\n",
    "#         proba_test = final_pipe.predict_proba(X_test)[:, pos_idx]\n",
    "\n",
    "#         auc_test = roc_auc_score(y_test, proba_test)\n",
    "#         ap_test  = average_precision_score(y_test, proba_test)\n",
    "#         y_pred   = final_pipe.predict(X_test)\n",
    "#         f1_test  = f1_score(y_test, y_pred, pos_label=pos_label)\n",
    "\n",
    "#         cm = confusion_matrix(y_test, y_pred, labels=classes_sorted)\n",
    "#         fpr, tpr, _   = roc_curve(y_test, proba_test, pos_label=pos_label)\n",
    "#         prec, rec, _ = precision_recall_curve(y_test, proba_test, pos_label=pos_label)\n",
    "\n",
    "#         test_confusions[model_name].append(cm)\n",
    "#         test_roc_curves[model_name].append((fpr, tpr, auc_test))\n",
    "#         test_pr_curves[model_name].append((rec, prec, ap_test))\n",
    "\n",
    "#         test_auc_list[model_name].append(float(auc_test))\n",
    "#         test_ap_list[model_name].append(float(ap_test))\n",
    "#         test_f1_list[model_name].append(float(f1_test))\n",
    "\n",
    "#         final_pipes[model_name][rep] = final_pipe\n",
    "\n",
    "#         ap_records.append({\n",
    "#             \"model\": model_name,\n",
    "#             \"rep\": rep,\n",
    "#             \"auc_test\": float(auc_test),\n",
    "#             \"ap_test\":  float(ap_test),\n",
    "#             \"f1_test\":  float(f1_test),\n",
    "#             \"val_f1\":   float(best_val_f1),\n",
    "#             \"val_ap\":   float(best_val_ap),\n",
    "#             \"val_auc\":  float(best_val_auc),\n",
    "#             \"clinical_context\": \"Control (0) vs Pediatric IIH (1)\",\n",
    "#             \"optimize_for\": OPTIMIZE_FOR\n",
    "#         })\n",
    "\n",
    "#         # ======================================================\n",
    "#         # SAVE MODEL (clinically descriptive filename)\n",
    "#         # ======================================================\n",
    "#         if SAVE_MODELS:\n",
    "#             safe_model = model_name.replace(\" \", \"\")\n",
    "#             fname = MODEL_DIR / (\n",
    "#                 f\"{safe_model}__rep{rep:02d}__PediatricIIH__OBJ{OPTIMIZE_FOR}\"\n",
    "#                 f\"__F1{f1_test:.4f}__AUC{auc_test:.4f}__AP{ap_test:.4f}.joblib\"\n",
    "#             )\n",
    "#             dump(final_pipe, fname, compress=3)\n",
    "#             saved_model_paths[model_name].append(str(fname))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746186f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# 6) Main Loop: Splits + Optuna + Test Evaluation (SAFE VERSION)\n",
    "# ===============================================================\n",
    "\n",
    "SAFE_K_CAP = 10     # <-- Bagging-KNN ve KNN için global güvenli sınır\n",
    "\n",
    "model_names = [\"Random Forest\", \"SVM\", \"MLP\", \"XGBoost\", \"KNN\", \"Bagging\"]\n",
    "\n",
    "best_hyperparameters    = {m: [] for m in model_names}\n",
    "selected_features_store = {m: {} for m in model_names}\n",
    "\n",
    "test_confusions = {m: [] for m in model_names}\n",
    "test_roc_curves = {m: [] for m in model_names}\n",
    "test_pr_curves  = {m: [] for m in model_names}\n",
    "\n",
    "test_auc_list = {m: [] for m in model_names}\n",
    "test_ap_list  = {m: [] for m in model_names}\n",
    "test_f1_list  = {m: [] for m in model_names}\n",
    "\n",
    "final_pipes = {m: {} for m in model_names}\n",
    "saved_model_paths = {m: [] for m in model_names}\n",
    "split_store = {}\n",
    "ap_records  = []\n",
    "\n",
    "\n",
    "for rep in range(N_REPS):\n",
    "\n",
    "    seed_outer = int(RANDOM_BASE + 10007*rep + 13)\n",
    "    seed_inner = int(RANDOM_BASE + 20011*rep + 97)\n",
    "    seed_model = int(RANDOM_BASE + 30029*rep + 211)\n",
    "    seed_tpe   = int(RANDOM_BASE + 40039*rep + 509)\n",
    "\n",
    "    # ======================================================\n",
    "    # OUTER (20% TEST)\n",
    "    # ======================================================\n",
    "    outer = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=TEST_FRACTION,\n",
    "        random_state=seed_outer\n",
    "    )\n",
    "    (trainval_idx, test_idx), = outer.split(X, y)\n",
    "\n",
    "    X_trainval = X.iloc[trainval_idx]\n",
    "    y_trainval = y[trainval_idx]\n",
    "\n",
    "    X_test = X.iloc[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "\n",
    "    split_store[rep] = {\n",
    "        \"trainval_idx\": trainval_idx.tolist(),\n",
    "        \"test_idx\": test_idx.tolist()\n",
    "    }\n",
    "\n",
    "    # ======================================================\n",
    "    # INNER\n",
    "    # ======================================================\n",
    "    inner = StratifiedShuffleSplit(\n",
    "        n_splits=1,\n",
    "        train_size=train_in_trainval,\n",
    "        test_size=val_in_trainval,\n",
    "        random_state=seed_inner\n",
    "    )\n",
    "    (tr_idx, val_idx), = inner.split(X_trainval, y_trainval)\n",
    "\n",
    "    X_train = X_trainval.iloc[tr_idx]\n",
    "    y_train = y_trainval[tr_idx]\n",
    "\n",
    "    X_val = X_trainval.iloc[val_idx]\n",
    "    y_val = y_trainval[val_idx]\n",
    "\n",
    "    PREPROC = build_preprocessor_all_robust(X_train)\n",
    "\n",
    "    # ======================================================\n",
    "    # OPTUNA LOOP\n",
    "    # ======================================================\n",
    "    for model_name in model_names:\n",
    "\n",
    "        study = optuna.create_study(\n",
    "            direction=\"maximize\",\n",
    "            sampler=optuna.samplers.TPESampler(seed=seed_tpe),\n",
    "            pruner=optuna.pruners.MedianPruner(n_warmup_steps=10)\n",
    "        )\n",
    "\n",
    "        study.optimize(\n",
    "            lambda tr: objective(\n",
    "                tr, model_name,\n",
    "                X_train, y_train,\n",
    "                X_val, y_val,\n",
    "                PREPROC,\n",
    "                seed=seed_model\n",
    "            ),\n",
    "            n_trials=N_TRIALS_BY_MODEL.get(model_name, 150),\n",
    "            n_jobs=1,\n",
    "        )\n",
    "\n",
    "        best_params_all = study.best_params.copy()\n",
    "        best_trial      = study.best_trial\n",
    "\n",
    "        best_val_f1  = float(best_trial.user_attrs[\"val_f1\"])\n",
    "        best_val_ap  = float(best_trial.user_attrs[\"val_ap\"])\n",
    "        best_val_auc = float(best_trial.user_attrs[\"val_auc\"])\n",
    "\n",
    "        best_hyperparameters[model_name].append({\n",
    "            \"rep\": rep,\n",
    "            \"best_params\": best_params_all.copy(),\n",
    "            \"val_f1\":  best_val_f1,\n",
    "            \"val_ap\":  best_val_ap,\n",
    "            \"val_auc\": best_val_auc,\n",
    "            \"optimize_for\": OPTIMIZE_FOR,\n",
    "            \"clinical_context\": \"Control (0) vs Pediatric IIH (1)\"\n",
    "        })\n",
    "\n",
    "        clean_params, base_choice = sanitize_params_for_final(model_name, best_params_all)\n",
    "\n",
    "        k_best = int(best_params_all.get(\"k_best\", min(K_MAX, X_train.shape[1])))\n",
    "\n",
    "        # ======================================================\n",
    "        # FINAL MODEL (SAFE KNN & SAFE BAGGING!)\n",
    "        # ======================================================\n",
    "\n",
    "        if model_name == \"Random Forest\":\n",
    "            clf_best = RandomForestClassifier(\n",
    "                **clean_params,\n",
    "                n_jobs=-1, class_weight=\"balanced\",\n",
    "                random_state=seed_model\n",
    "            )\n",
    "\n",
    "        elif model_name == \"SVM\":\n",
    "            clf_best = SVC(\n",
    "                **clean_params,\n",
    "                probability=True, class_weight=\"balanced\",\n",
    "                random_state=seed_model\n",
    "            )\n",
    "\n",
    "        elif model_name == \"MLP\":\n",
    "            clf_best = MLPClassifier(**clean_params, random_state=seed_model)\n",
    "\n",
    "        elif model_name == \"XGBoost\":\n",
    "            clf_best = XGBClassifier(\n",
    "                **clean_params,\n",
    "                eval_metric=\"logloss\",\n",
    "                n_jobs=-1, random_state=seed_model\n",
    "            )\n",
    "\n",
    "        elif model_name == \"KNN\":\n",
    "            # ABSOLUTE SAFETY\n",
    "            raw_k = clean_params.get(\"n_neighbors\", 5)\n",
    "            safe_k = min(raw_k, max(1, len(X_train) - 1), SAFE_K_CAP)\n",
    "            clean_params[\"n_neighbors\"] = safe_k\n",
    "\n",
    "            clf_best = KNeighborsClassifier(**clean_params)\n",
    "\n",
    "        elif model_name == \"Bagging\":\n",
    "\n",
    "            # --------------------------------------------------\n",
    "            # BASE ESTIMATOR FIX\n",
    "            # --------------------------------------------------\n",
    "            if base_choice is None:\n",
    "                base_choice = best_params_all.get(\"base_estimator\", \"tree\")\n",
    "\n",
    "            if base_choice == \"tree\":\n",
    "                tree_params = {\n",
    "                    k: best_params_all[k]\n",
    "                    for k in best_params_all\n",
    "                    if k in [\"max_depth\",\"min_samples_split\",\"min_samples_leaf\",\n",
    "                             \"criterion\",\"splitter\",\"max_features\"]\n",
    "                }\n",
    "                base_est = DecisionTreeClassifier(\n",
    "                    **tree_params, class_weight=\"balanced\",\n",
    "                    random_state=seed_model\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                # SAFE BAGGING KNN\n",
    "                raw_k = best_params_all.get(\"n_neighbors\", 5)\n",
    "                safe_k = min(raw_k, SAFE_K_CAP, max(1, len(X_train) - 1))\n",
    "\n",
    "                knn_params = {\n",
    "                    k: best_params_all[k]\n",
    "                    for k in best_params_all\n",
    "                    if k in [\"weights\",\"p\",\"leaf_size\",\"metric\"]\n",
    "                }\n",
    "                knn_params[\"n_neighbors\"] = safe_k\n",
    "\n",
    "                base_est = KNeighborsClassifier(**knn_params)\n",
    "\n",
    "            # --------------------------------------------------\n",
    "            # BAGGING PARAMETERS\n",
    "            # --------------------------------------------------\n",
    "            bag_params = {\n",
    "                k: best_params_all[k]\n",
    "                for k in best_params_all\n",
    "                if k in [\"n_estimators\",\"max_samples\",\"max_features\",\n",
    "                         \"bootstrap\",\"bootstrap_features\",\"n_jobs\",\"random_state\"]\n",
    "            }\n",
    "\n",
    "            clf_best = BaggingClassifier(\n",
    "                estimator=base_est,\n",
    "                random_state=seed_model,\n",
    "                **bag_params\n",
    "            )\n",
    "\n",
    "        # ======================================================\n",
    "        # FINAL PIPELINE\n",
    "        # ======================================================\n",
    "        selector_final = SelectKBest(score_func=f_classif, k=k_best)\n",
    "\n",
    "        final_pipe = Pipeline([\n",
    "            (\"prep\", PREPROC),\n",
    "            (\"feat\", selector_final),\n",
    "            (\"clf\", clf_best)\n",
    "        ])\n",
    "\n",
    "        final_pipe.fit(X_trainval, y_trainval)\n",
    "        final_pipes[model_name][rep] = final_pipe\n",
    "\n",
    "        # ======================================================\n",
    "        # STORE SELECTED FEATURES\n",
    "        # ======================================================\n",
    "        mask, sel_names, sel_names_clean, base_names = get_selected_feature_info(final_pipe, X)\n",
    "\n",
    "        selected_features_store[model_name][rep] = {\n",
    "            \"k_best\": k_best,\n",
    "            \"selected_feature_names_clean\": sel_names_clean\n",
    "        }\n",
    "\n",
    "        # ======================================================\n",
    "        # TEST METRICS\n",
    "        # ======================================================\n",
    "        pos_idx = int(np.where(final_pipe.named_steps[\"clf\"].classes_ == pos_label)[0][0])\n",
    "\n",
    "        proba_test = final_pipe.predict_proba(X_test)[:, pos_idx]\n",
    "\n",
    "        auc_test = roc_auc_score(y_test, proba_test)\n",
    "        ap_test  = average_precision_score(y_test, proba_test)\n",
    "        y_pred   = final_pipe.predict(X_test)\n",
    "        f1_test  = f1_score(y_test, y_pred, pos_label=pos_label)\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=classes_sorted)\n",
    "        fpr, tpr, _ = roc_curve(y_test, proba_test, pos_label=pos_label)\n",
    "        prec, rec, _ = precision_recall_curve(y_test, proba_test, pos_label=pos_label)\n",
    "\n",
    "        test_confusions[model_name].append(cm)\n",
    "        test_roc_curves[model_name].append((fpr, tpr, auc_test))\n",
    "        test_pr_curves[model_name].append((rec, prec, ap_test))\n",
    "\n",
    "        test_auc_list[model_name].append(float(auc_test))\n",
    "        test_ap_list[model_name].append(float(ap_test))\n",
    "        test_f1_list[model_name].append(float(f1_test))\n",
    "\n",
    "        ap_records.append({\n",
    "            \"model\": model_name,\n",
    "            \"rep\": rep,\n",
    "            \"auc_test\": float(auc_test),\n",
    "            \"ap_test\":  float(ap_test),\n",
    "            \"f1_test\":  float(f1_test),\n",
    "            \"val_f1\":   float(best_val_f1),\n",
    "            \"val_ap\":   float(best_val_ap),\n",
    "            \"val_auc\":  float(best_val_auc),\n",
    "            \"clinical_context\": \"Control (0) vs Pediatric IIH (1)\",\n",
    "            \"optimize_for\": OPTIMIZE_FOR\n",
    "        })\n",
    "\n",
    "        # ======================================================\n",
    "        # SAVE MODEL\n",
    "        # ======================================================\n",
    "        if SAVE_MODELS:\n",
    "            safe_model = model_name.replace(\" \", \"\")\n",
    "            fname = MODEL_DIR / (\n",
    "                f\"{safe_model}__rep{rep:02d}__PediatricIIH__OBJ{OPTIMIZE_FOR}\"\n",
    "                f\"__F1{f1_test:.4f}__AUC{auc_test:.4f}__AP{ap_test:.4f}.joblib\"\n",
    "            )\n",
    "            dump(final_pipe, fname, compress=3)\n",
    "            saved_model_paths[model_name].append(str(fname))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7bac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# 7) Run-level Tables, Summaries, ROC/PR(mean±std)\n",
    "# ===============================================================\n",
    "\n",
    "# ---- Save raw run-level F1 and AP scores ----\n",
    "df_scores = pd.DataFrame(ap_records).sort_values([\"model\", \"f1_test\"]).reset_index(drop=True)\n",
    "df_scores.to_csv(OUT_DIR / \"scores_f1_pediatric_iih.csv\", index=False)\n",
    "\n",
    "df_scores_ap = pd.DataFrame(ap_records).sort_values([\"model\", \"ap_test\"]).reset_index(drop=True)\n",
    "df_scores_ap.to_csv(OUT_DIR / \"scores_ap_pediatric_iih.csv\", index=False)\n",
    "\n",
    "# ---- Summary table (mean±std) for AUC / AP / F1 ----\n",
    "rows_sum = []\n",
    "for m in model_names:\n",
    "    rows_sum.append({\n",
    "        \"model\":   m,\n",
    "        \"AUC_mean\": np.mean(test_auc_list[m]) if test_auc_list[m] else np.nan,\n",
    "        \"AUC_std\":  np.std(test_auc_list[m], ddof=1) if len(test_auc_list[m]) > 1 else 0.0,\n",
    "        \"AP_mean\":  np.mean(test_ap_list[m]) if test_ap_list[m] else np.nan,\n",
    "        \"AP_std\":   np.std(test_ap_list[m], ddof=1) if len(test_ap_list[m]) > 1 else 0.0,\n",
    "        \"F1_mean\":  np.mean(test_f1_list[m]) if test_f1_list[m] else np.nan,\n",
    "        \"F1_std\":   np.std(test_f1_list[m], ddof=1) if len(test_f1_list[m]) > 1 else 0.0,\n",
    "        \"N_reps\":   len(test_auc_list[m])\n",
    "    })\n",
    "df_summary = pd.DataFrame(rows_sum)\n",
    "df_summary.to_csv(OUT_DIR / \"summary_pediatric_iih_mean_std.csv\", index=False)\n",
    "\n",
    "# ---- Save hyperparameters and feature selection info ----\n",
    "with open(OUT_DIR / \"best_hyperparameters.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(best_hyperparameters, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(OUT_DIR / \"selected_features_store.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(selected_features_store, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "dump({\n",
    "    \"test_confusions\": test_confusions,\n",
    "    \"test_roc_curves\": test_roc_curves,\n",
    "    \"test_pr_curves\":  test_pr_curves,\n",
    "    \"test_auc_list\":   test_auc_list,\n",
    "    \"test_ap_list\":    test_ap_list,\n",
    "    \"test_f1_list\":    test_f1_list,\n",
    "    \"split_store\":     split_store,\n",
    "    \"saved_model_paths\": saved_model_paths\n",
    "}, OUT_DIR / \"diagnostics_pediatric_iih.joblib\", compress=3)\n",
    "\n",
    "# ===============================================================\n",
    "#                 ROC CURVE (Mean ± Std)\n",
    "# ===============================================================\n",
    "fpr_grid = np.linspace(0, 1, 201)\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for m, curves in test_roc_curves.items():\n",
    "    if not curves:\n",
    "        continue\n",
    "    interp_tprs = []\n",
    "    for (fpr, tpr, _) in curves:\n",
    "        tpr_i = np.interp(fpr_grid, fpr, tpr)\n",
    "        tpr_i[0], tpr_i[-1] = 0.0, 1.0\n",
    "        interp_tprs.append(tpr_i)\n",
    "\n",
    "    interp_tprs = np.vstack(interp_tprs)\n",
    "    mean_tpr = interp_tprs.mean(axis=0)\n",
    "    std_tpr  = interp_tprs.std(axis=0, ddof=1)\n",
    "\n",
    "    mean_auc = float(np.mean(test_auc_list[m]))\n",
    "    std_auc  = float(np.std(test_auc_list[m], ddof=1))\n",
    "\n",
    "    plt.plot(fpr_grid, mean_tpr,\n",
    "             label=f\"{m} (AUC={mean_auc:.3f}±{std_auc:.3f})\", linewidth=2)\n",
    "    plt.fill_between(\n",
    "        fpr_grid,\n",
    "        np.maximum(mean_tpr - std_tpr, 0),\n",
    "        np.minimum(mean_tpr + std_tpr, 1),\n",
    "        alpha=0.15\n",
    "    )\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', linewidth=1)\n",
    "plt.xlabel(\"False Positive Rate (Control → IIH misclassification)\")\n",
    "plt.ylabel(\"True Positive Rate (Sensitivity for Pediatric IIH)\")\n",
    "plt.title(\"Pediatric IIH vs Controls — ROC Curve (Mean ± 1 SD, Test Set)\", fontsize=15, fontweight=\"semibold\")\n",
    "plt.legend(loc='lower right'); plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"roc_pediatric_iih_mean_std.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# ===============================================================\n",
    "#                 PR CURVE (Mean ± Std)\n",
    "# ===============================================================\n",
    "recall_grid = np.linspace(0, 1, 201)\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for m, curves in test_pr_curves.items():\n",
    "    if not curves:\n",
    "        continue\n",
    "    interp_precs = []\n",
    "    for (rec, prec, _) in curves:\n",
    "        order = np.argsort(rec)\n",
    "        rec_sorted = rec[order]\n",
    "        prec_sorted = prec[order]\n",
    "        prec_i = np.interp(\n",
    "            recall_grid,\n",
    "            rec_sorted,\n",
    "            prec_sorted,\n",
    "            left=prec_sorted[0],\n",
    "            right=prec_sorted[-1]\n",
    "        )\n",
    "        interp_precs.append(prec_i)\n",
    "\n",
    "    interp_precs = np.vstack(interp_precs)\n",
    "    mean_prec = interp_precs.mean(axis=0)\n",
    "    std_prec  = interp_precs.std(axis=0, ddof=1)\n",
    "\n",
    "    mean_ap = float(np.mean(test_ap_list[m]))\n",
    "    std_ap  = float(np.std(test_ap_list[m], ddof=1))\n",
    "\n",
    "    plt.plot(recall_grid, mean_prec,\n",
    "             label=f\"{m} (AP={mean_ap:.3f}±{std_ap:.3f})\", linewidth=2)\n",
    "    plt.fill_between(\n",
    "        recall_grid,\n",
    "        np.maximum(mean_prec - std_prec, 0),\n",
    "        np.minimum(mean_prec + std_prec, 1),\n",
    "        alpha=0.15\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Recall (Sensitivity for Pediatric IIH)\")\n",
    "plt.ylabel(\"Precision (Positive Predictive Value)\")\n",
    "plt.title(\"Pediatric IIH vs Controls — Precision–Recall Curve (Mean ± 1 SD, Test Set)\", fontsize=15, fontweight=\"semibold\")\n",
    "plt.legend(loc='lower left'); plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"pr_pediatric_iih_mean_std.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n[OK] Pediatric IIH run-level outputs saved under:\", OUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66322a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# 8) Confusion Matrices (All runs) + Mean±Std Visualizations\n",
    "# ===============================================================\n",
    "rows_all = []\n",
    "for model_name, cm_list in test_confusions.items():\n",
    "    for rep0, cm in enumerate(cm_list):\n",
    "        cm = np.asarray(cm)\n",
    "        if cm.shape != (2, 2):\n",
    "            raise ValueError(f\"[{model_name}] rep={rep0}: Confusion matrix shape {cm.shape} is not 2x2.\")\n",
    "        tn, fp = int(cm[0, 0]), int(cm[0, 1])\n",
    "        fn, tp = int(cm[1, 0]), int(cm[1, 1])\n",
    "        rows_all.append({\n",
    "            \"model\": model_name,\n",
    "            \"run\": rep0 + 1,\n",
    "            \"tn\": tn,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn,\n",
    "            \"tp\": tp,\n",
    "            \"clinical_label_neg\": \"Control\",\n",
    "            \"clinical_label_pos\": \"Pediatric IIH\"\n",
    "        })\n",
    "\n",
    "df_conf_all = pd.DataFrame(rows_all, columns=[\n",
    "    \"model\", \"run\", \"tn\", \"fp\", \"fn\", \"tp\",\n",
    "    \"clinical_label_neg\", \"clinical_label_pos\"\n",
    "])\n",
    "\n",
    "df_conf_all.to_excel(\n",
    "    BASE_OUT_DIR / f\"unseen_confusions_allruns_{RUN_TAG}.xlsx\",\n",
    "    sheet_name=\"confusions_allruns\",\n",
    "    index=False\n",
    ")\n",
    "df_conf_all.to_csv(\n",
    "    BASE_OUT_DIR / f\"unseen_confusions_allruns_{RUN_TAG}.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "# ===============================================================\n",
    "# Mean±std confusion plots (raw, row-normalized, all-normalized)\n",
    "# ===============================================================\n",
    "\n",
    "clinical_classes = [\"Control\", \"Pediatric IIH\"]\n",
    "\n",
    "for norm_tag in [None, \"row\", \"all\"]:\n",
    "    agg = aggregate_cm_mean_std(df_conf_all, normalize=norm_tag)\n",
    "    out_png = BASE_OUT_DIR / f\"confusion_mean_std_{RUN_TAG}_{'raw' if norm_tag is None else norm_tag}.png\"\n",
    "\n",
    "    # IMPORTANT: use clinical labels, NOT numeric labels\n",
    "    plot_confusion_mean_std(\n",
    "        agg,\n",
    "        normalize=norm_tag,\n",
    "        classes=clinical_classes,\n",
    "        out_path=out_png\n",
    "    )\n",
    "    print(f\"[OK] Saved: {out_png}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24078a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# 9) Per-run Metrics + Friedman/Wilcoxon + Significance (Holm-gated)\n",
    "# ===============================================================\n",
    "ALPHA_SIG = 0.05      # Holm-corrected alpha\n",
    "TIE_EPS   = 1e-12     # tie threshold for mean_diff\n",
    "\n",
    "# Per-run metrics table\n",
    "metrics_list = ['sensitivity','specificity','precision','recall','f1','mcc','gmean','accuracy']\n",
    "mets = df_conf_all.apply(lambda r: compute_row_metrics(r['tn'], r['fp'], r['fn'], r['tp']), axis=1)\n",
    "(df_conf_all['sensitivity'], df_conf_all['specificity'], df_conf_all['precision'], df_conf_all['recall'],\n",
    " df_conf_all['f1'], df_conf_all['mcc'], df_conf_all['gmean'], df_conf_all['accuracy']) = zip(*mets)\n",
    "\n",
    "df_per_run = df_conf_all[['model','run','tn','fp','fn','tp'] + metrics_list].copy()\n",
    "df_per_run.to_csv(OUT_DIR / \"metrics_per_run_pediatric_iih.csv\", index=False)\n",
    "\n",
    "# Summary (mean ± std)\n",
    "summary_20 = (df_per_run.groupby('model')[metrics_list].agg(['mean','std','count']).reset_index())\n",
    "summary_20.columns = ['model'] + ['_'.join(col).strip() for col in summary_20.columns[1:]]\n",
    "summary_20.to_csv(OUT_DIR / \"metrics_summary_by_model_pediatric_iih.csv\", index=False)\n",
    "\n",
    "# Friedman + pairwise Wilcoxon(Holm)\n",
    "friedman_rows = []\n",
    "pairwise_all  = []\n",
    "\n",
    "for mtr in metrics_list:\n",
    "    df_long = df_per_run[['model','run', mtr]].copy()\n",
    "    res, pw_raw = friedman_and_pairwise(df_long, mtr)\n",
    "\n",
    "    pw = pw_raw.copy()\n",
    "\n",
    "    # Direction raw\n",
    "    pw['direction_raw'] = np.where(\n",
    "        pw['mean_diff'] >  TIE_EPS, pw['model_a'],\n",
    "        np.where(pw['mean_diff'] < -TIE_EPS, pw['model_b'], 'tie')\n",
    "    )\n",
    "\n",
    "    # Holm correction\n",
    "    mask = ~pw['p_raw'].isna()\n",
    "    if mask.any():\n",
    "        _, p_holm, _, _ = multipletests(pw.loc[mask, 'p_raw'].values, method='holm')\n",
    "        pw.loc[mask, 'p_holm'] = p_holm\n",
    "    else:\n",
    "        pw['p_holm'] = np.nan\n",
    "\n",
    "    # Winner gating\n",
    "    winner_report = []\n",
    "    for _, r in pw.iterrows():\n",
    "        if pd.isna(r['p_holm']):\n",
    "            winner_report.append(\"None\")\n",
    "        elif r['p_holm'] < ALPHA_SIG:\n",
    "            if r['mean_diff'] >  TIE_EPS: winner_report.append(r['model_a'])\n",
    "            elif r['mean_diff'] < -TIE_EPS: winner_report.append(r['model_b'])\n",
    "            else: winner_report.append(\"No\")\n",
    "        else:\n",
    "            winner_report.append(\"No\")\n",
    "    pw['winner'] = winner_report\n",
    "\n",
    "    friedman_rows.append({\n",
    "        'metric': mtr,\n",
    "        'friedman_stat': res['friedman']['stat'],\n",
    "        'friedman_p': res['friedman']['p'],\n",
    "        'k_models': res['friedman']['k'],\n",
    "        'n_runs': res['friedman']['n']\n",
    "    })\n",
    "    pairwise_all.append(pw)\n",
    "\n",
    "df_friedman_20 = pd.DataFrame(friedman_rows)\n",
    "df_pairwise_20 = pd.concat(pairwise_all, ignore_index=True)\n",
    "\n",
    "# Save CSVs\n",
    "df_friedman_20.to_csv(OUT_DIR / \"stats_friedman_pediatric_iih.csv\", index=False)\n",
    "df_pairwise_20.to_csv(OUT_DIR / \"stats_pairwise_pediatric_iih_directional.csv\", index=False)\n",
    "print(\"[OK] Saved significance stats (Friedman + Wilcoxon-Holm).\")\n",
    "\n",
    "# ===============================================================\n",
    "# Build model×model significance matrices\n",
    "# ===============================================================\n",
    "signif_xlsx = BASE_OUT_DIR / f\"pairwise_matrices_pediatric_iih_{RUN_TAG}.xlsx\"\n",
    "models_all = sorted(set(df_pairwise_20[\"model_a\"]) | set(df_pairwise_20[\"model_b\"]))\n",
    "\n",
    "def _plot_matrix_text(M_df, title, out_png, cmap=None, highlight_mask=None):\n",
    "    code = np.zeros(M_df.shape, dtype=float)\n",
    "    txt = M_df.values.astype(str)\n",
    "    for i in range(M_df.shape[0]):\n",
    "        for j in range(M_df.shape[1]):\n",
    "            if i == j:\n",
    "                code[i, j] = np.nan\n",
    "            else:\n",
    "                code[i, j] = 1.0 if highlight_mask is not None and highlight_mask[i, j] else 0.0\n",
    "\n",
    "    plt.figure(figsize=(1.2*M_df.shape[1], 1.0*M_df.shape[0]))\n",
    "    ax = sns.heatmap(\n",
    "        code, annot=txt, fmt=\"\",\n",
    "        cbar=False,\n",
    "        cmap=cmap if cmap else sns.color_palette([\"#f0f0f0\",\"#bfe3bf\"], as_cmap=True),\n",
    "        linewidths=.5, linecolor='white'\n",
    "    )\n",
    "    ax.set_xticklabels(M_df.columns, rotation=45, ha='right')\n",
    "    ax.set_yticklabels(M_df.index, rotation=0)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "# Write matrices\n",
    "with pd.ExcelWriter(signif_xlsx) as writer:\n",
    "    for mtr in metrics_list:\n",
    "        sub = df_pairwise_20[df_pairwise_20[\"metric\"] == mtr].copy()\n",
    "\n",
    "        P = pd.DataFrame(np.nan, index=models_all, columns=models_all)\n",
    "        M = pd.DataFrame(\"None\", index=models_all, columns=models_all)\n",
    "\n",
    "        for _, r in sub.iterrows():\n",
    "            a, b, p, w = r[\"model_a\"], r[\"model_b\"], r[\"p_holm\"], r[\"winner\"]\n",
    "            P.loc[a, b] = P.loc[b, a] = p\n",
    "            M.loc[a, b] = M.loc[b, a] = w\n",
    "\n",
    "        np.fill_diagonal(P.values, 0.0)\n",
    "        np.fill_diagonal(M.values, \"-\")\n",
    "\n",
    "        P.to_excel(writer, sheet_name=f\"{mtr}_pHolm\")\n",
    "        M.to_excel(writer, sheet_name=f\"{mtr}_sig\")\n",
    "\n",
    "        # pHolm heatmap\n",
    "        P_plot = P.copy()\n",
    "        with np.errstate(divide='ignore'):\n",
    "            P_plot = -np.log10(P_plot)\n",
    "\n",
    "        plt.figure(figsize=(1.2*len(models_all), 1.0*len(models_all)))\n",
    "        ax = sns.heatmap(\n",
    "            P_plot, cmap=\"mako\",\n",
    "            linewidths=.5, linecolor='white',\n",
    "            cbar_kws={'label': '-log10(pHolm)'}\n",
    "        )\n",
    "        ax.set_xticklabels(P_plot.columns, rotation=45, ha='right')\n",
    "        ax.set_yticklabels(P_plot.index, rotation=0)\n",
    "        plt.title(f\"{mtr} — Holm-Corrected Significance (Pediatric IIH)\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(STATS_PLOTS / f\"{mtr}_pHolm_heatmap_pediatric_iih_{RUN_TAG}.png\", dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "        # Winner matrix\n",
    "        hl = np.zeros(M.shape, dtype=bool)\n",
    "        for i in range(M.shape[0]):\n",
    "            for j in range(M.shape[1]):\n",
    "                val = M.iloc[i, j]\n",
    "                hl[i, j] = (i != j and val not in {\"No\", \"None\", \"-\"})\n",
    "\n",
    "        _plot_matrix_text(\n",
    "            M,\n",
    "            title=f\"{mtr} — Pairwise Model Superiority (Holm-gated) — Pediatric IIH\",\n",
    "            out_png=STATS_PLOTS / f\"{mtr}_sig_winner_pediatric_iih_{RUN_TAG}.png\",\n",
    "            highlight_mask=hl\n",
    "        )\n",
    "\n",
    "print(f\"[OK] Saved Pediatric IIH significance matrices & plots: {signif_xlsx}, {STATS_PLOTS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4026d6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# 10) 95% t-CI for ALL metrics + AUC/AP + Paired t-tests (AUC)\n",
    "# Context: Pediatric IIH vs Control classification\n",
    "# ===============================================================\n",
    "ALPHA_CI = 0.05\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# CI for per-run metrics (sensitivity, specificity, precision, ...)\n",
    "# ---------------------------------------------------------------\n",
    "rows_ci = []\n",
    "for model_name, g in df_per_run.groupby('model'):\n",
    "    for mtr in metrics_list:\n",
    "        m, s, lo, hi, n = mean_std_ci_t(g[mtr].values, alpha=ALPHA_CI)\n",
    "        rows_ci.append({\n",
    "            'model': model_name,\n",
    "            'metric': mtr,\n",
    "            'mean': m, 'std': s,\n",
    "            'ci95_low': lo, 'ci95_high': hi,\n",
    "            'n_runs_used': n\n",
    "        })\n",
    "\n",
    "df_ci_long = pd.DataFrame(rows_ci).sort_values(['model','metric']).reset_index(drop=True)\n",
    "df_ci_long.to_csv(\n",
    "    OUT_DIR / \"metrics_ci95_by_model_pediatric_iih_long.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "# Wide variant ---------------------------------------------------\n",
    "wide_rows = []\n",
    "for model_name, g in df_per_run.groupby('model'):\n",
    "    row = {'model': model_name}\n",
    "    for mtr in metrics_list:\n",
    "        m, s, lo, hi, n = mean_std_ci_t(g[mtr].values, alpha=ALPHA_CI)\n",
    "        row[f\"{mtr}_mean\"] = m\n",
    "        row[f\"{mtr}_std\"] = s\n",
    "        row[f\"{mtr}_ci95_low\"] = lo\n",
    "        row[f\"{mtr}_ci95_high\"] = hi\n",
    "        row[f\"{mtr}_n\"] = n\n",
    "    wide_rows.append(row)\n",
    "\n",
    "df_ci_wide = pd.DataFrame(wide_rows).sort_values('model').reset_index(drop=True)\n",
    "df_ci_wide.to_csv(\n",
    "    OUT_DIR / \"metrics_ci95_by_model_pediatric_iih_wide.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# CI for AUC and AP (per model)\n",
    "# ---------------------------------------------------------------\n",
    "rows_auc_ap = []\n",
    "auc_src = {k: list(v) for k, v in test_auc_list.items()}\n",
    "ap_src  = {k: list(v) for k, v in test_ap_list.items()}\n",
    "\n",
    "for model, arr in auc_src.items():\n",
    "    m, s, lo, hi, n = mean_std_ci_t(arr, alpha=ALPHA_CI)\n",
    "    rows_auc_ap.append({\n",
    "        'model': model,\n",
    "        'metric': 'auc',\n",
    "        'mean': m, 'std': s,\n",
    "        'ci95_low': lo, 'ci95_high': hi,\n",
    "        'n_runs_used': n\n",
    "    })\n",
    "\n",
    "for model, arr in ap_src.items():\n",
    "    m, s, lo, hi, n = mean_std_ci_t(arr, alpha=ALPHA_CI)\n",
    "    rows_auc_ap.append({\n",
    "        'model': model,\n",
    "        'metric': 'ap',\n",
    "        'mean': m, 'std': s,\n",
    "        'ci95_low': lo, 'ci95_high': hi,\n",
    "        'n_runs_used': n\n",
    "    })\n",
    "\n",
    "if rows_auc_ap:\n",
    "    pd.DataFrame(rows_auc_ap).sort_values(\n",
    "        ['model', 'metric']\n",
    "    ).to_csv(\n",
    "        OUT_DIR / \"auc_ap_ci95_by_model_pediatric_iih.csv\",\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Paired t-tests for AUC (Model A vs Model B)\n",
    "# ---------------------------------------------------------------\n",
    "pairs = []\n",
    "models_auc = list(auc_src.keys())\n",
    "\n",
    "for a, b in itertools.combinations(models_auc, 2):\n",
    "    res = paired_ttest_ci(auc_src[a], auc_src[b], alpha=ALPHA_CI)\n",
    "    pairs.append({\n",
    "        'model_a': a,\n",
    "        'model_b': b,\n",
    "        'n_pairs': res['n'],\n",
    "        'delta_mean_auc_(a-b)': res['mean_diff'],\n",
    "        'ci95_low': res['ci95'][0],\n",
    "        'ci95_high': res['ci95'][1],\n",
    "        't_stat': res['t'],\n",
    "        'p_value': res['p']\n",
    "    })\n",
    "\n",
    "df_pairwise_auc = pd.DataFrame(pairs)\n",
    "\n",
    "if not df_pairwise_auc.empty:\n",
    "    mask = ~df_pairwise_auc['p_value'].isna()\n",
    "    if mask.any():\n",
    "        _, p_holm, _, _ = multipletests(\n",
    "            df_pairwise_auc.loc[mask, 'p_value'].values,\n",
    "            method='holm'\n",
    "        )\n",
    "        df_pairwise_auc.loc[mask, 'p_value_holm'] = p_holm\n",
    "    else:\n",
    "        df_pairwise_auc['p_value_holm'] = np.nan\n",
    "\n",
    "    df_pairwise_auc.to_csv(\n",
    "        OUT_DIR / \"auc_pairwise_paired_ttest_pediatric_iih.csv\",\n",
    "        index=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f2e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# 11) XAI: SHAP & LIME (SELECTED feature space) – Pediatric IIH vs Controls\n",
    "# ===============================================================\n",
    "\n",
    "if DO_EXPLAINERS:\n",
    "    try:\n",
    "        import shap\n",
    "        from lime.lime_tabular import LimeTabularExplainer\n",
    "        from scipy import sparse as sp\n",
    "        import matplotlib as mpl\n",
    "        import matplotlib.pyplot as plt\n",
    "        mpl.rcParams['figure.max_open_warning'] = 0\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Explainability packages not available; SHAP/LIME skipped: {e}\")\n",
    "        DO_EXPLAINERS = False\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# CLASS NAMES MUST BE CLINICAL, NOT NUMERIC\n",
    "# ---------------------------------------------------------------\n",
    "CLINICAL_CLASS_NAMES = [\"Control\", \"Pediatric IIH\"]\n",
    "\n",
    "if DO_EXPLAINERS:\n",
    "\n",
    "    def transform_until_estimator(pipe: Pipeline, X_in):\n",
    "        Xt = X_in\n",
    "        for name, step in pipe.named_steps.items():\n",
    "            if name == 'clf':\n",
    "                break\n",
    "            Xt = step.transform(Xt)\n",
    "            if sp.issparse(Xt):\n",
    "                Xt = Xt.toarray()\n",
    "        return Xt\n",
    "\n",
    "    def get_selected_names_for_pipe(pipe: Pipeline, X_ref):\n",
    "        mask, sel_names, sel_names_clean, _ = get_selected_feature_info(pipe, X_ref)\n",
    "        return sel_names, sel_names_clean\n",
    "\n",
    "    # Select mid-AP runs\n",
    "    def select_median_window(df_scores, model_name, window=2):\n",
    "        dfm = df_scores[df_scores['model'] == model_name].sort_values('ap_test').reset_index(drop=True)\n",
    "        if dfm.empty:\n",
    "            return dfm\n",
    "        k = len(dfm); mid = k // 2\n",
    "        lo = max(0, mid - window); hi = min(k, mid + window + 1)\n",
    "        return dfm.iloc[lo:hi][['model','rep','ap_test','auc_test']]\n",
    "\n",
    "    WINDOW = 2\n",
    "    df_scores_ap = pd.DataFrame(ap_records)\n",
    "    selected_rows = {m: select_median_window(df_scores_ap, m, window=WINDOW) for m in model_names}\n",
    "\n",
    "    # Save info on selected runs\n",
    "    sel_log = []\n",
    "    for m, d in selected_rows.items():\n",
    "        if d is None or d.empty:\n",
    "            continue\n",
    "        for _, r in d.iterrows():\n",
    "            sel_log.append({\n",
    "                'model': m,\n",
    "                'rep': int(r['rep']),\n",
    "                'ap_test': float(r.get('ap_test', np.nan)),\n",
    "                'auc_test': float(r.get('auc_test', np.nan))\n",
    "            })\n",
    "    if sel_log:\n",
    "        pd.DataFrame(sel_log).to_csv(EXPL_DIR / \"selected_runs_for_explanations.csv\", index=False)\n",
    "\n",
    "    # Background & test subsample sizes\n",
    "    BGN = 200\n",
    "    TSN = 50\n",
    "    TOPK_DEP = 5\n",
    "\n",
    "    rng = np.random.default_rng(42)\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # SHAP helper for tree models\n",
    "    # ---------------------------------------------------------------\n",
    "    def compute_shap_2d_tree(clf, X_bg_sel_np, X_ts_sel_np, pos_idx_local):\n",
    "        try:\n",
    "            K = min(100, max(10, X_bg_sel_np.shape[0] // 2))\n",
    "            bg = shap.kmeans(X_bg_sel_np, K)\n",
    "        except Exception:\n",
    "            idx = rng.choice(X_bg_sel_np.shape[0], size=min(100, X_bg_sel_np.shape[0]), replace=False)\n",
    "            bg = X_bg_sel_np[idx]\n",
    "\n",
    "        explainer = shap.TreeExplainer(\n",
    "            clf,\n",
    "            data=bg,\n",
    "            feature_perturbation=\"interventional\",\n",
    "            model_output=\"probability\"\n",
    "        )\n",
    "        shap_values = explainer.shap_values(X_ts_sel_np)\n",
    "\n",
    "        # Unify output shape\n",
    "        if isinstance(shap_values, list):\n",
    "            sv = shap_values[pos_idx_local]\n",
    "        else:\n",
    "            sv = shap_values\n",
    "            if sv.ndim == 3 and sv.shape[-1] == len(clf.classes_):\n",
    "                sv = sv[:, :, pos_idx_local]\n",
    "            elif sv.ndim != 2:\n",
    "                raise RuntimeError(f\"Unexpected SHAP shape: {sv.shape}\")\n",
    "\n",
    "        return sv\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # MAIN LOOP: per-model, per-selected-rep explainability\n",
    "    # ---------------------------------------------------------------\n",
    "    for model_name in model_names:\n",
    "\n",
    "        sel_df = selected_rows.get(model_name, pd.DataFrame())\n",
    "        if sel_df is None or sel_df.empty:\n",
    "            print(f\"[WARN] {model_name}: no selected runs; skipping.\")\n",
    "            continue\n",
    "\n",
    "        safe_model = model_name.replace(\" \", \"\")\n",
    "\n",
    "        for _, row in sel_df.iterrows():\n",
    "            rep = int(row['rep'])\n",
    "            if rep not in split_store:\n",
    "                print(f\"[WARN] {model_name}: rep={rep} missing; skipped.\")\n",
    "                continue\n",
    "\n",
    "            idx_trainval = split_store[rep]['trainval_idx']\n",
    "            idx_test = split_store[rep]['test_idx']\n",
    "            X_bg_full = X.iloc[idx_trainval]\n",
    "            X_ts_full = X.iloc[idx_test]\n",
    "\n",
    "            # Subsample for speed\n",
    "            b_idx = rng.choice(len(X_bg_full), size=min(BGN, len(X_bg_full)), replace=False)\n",
    "            t_idx = rng.choice(len(X_ts_full), size=min(TSN, len(X_ts_full)), replace=False)\n",
    "            X_bg = X_bg_full.iloc[b_idx]\n",
    "            X_ts = X_ts_full.iloc[t_idx]\n",
    "\n",
    "            pipe = final_pipes[model_name][rep]\n",
    "            clf  = pipe.named_steps['clf']\n",
    "\n",
    "            pos_idx_local = int(np.where(clf.classes_ == pos_label)[0][0])\n",
    "\n",
    "            # Transform into selected-feature space\n",
    "            X_bg_sel = transform_until_estimator(pipe, X_bg)\n",
    "            X_ts_sel = transform_until_estimator(pipe, X_ts)\n",
    "            feat_names_sel, feat_names_sel_clean = get_selected_names_for_pipe(pipe, X)\n",
    "            feat_names_sel_clean = [f.split(\"__\")[-1] for f in feat_names_sel_clean]\n",
    "\n",
    "            X_bg_np = np.asarray(X_bg_sel, dtype=float)\n",
    "            X_ts_np = np.asarray(X_ts_sel, dtype=float)\n",
    "\n",
    "            model_type = type(clf).__name__.lower()\n",
    "            is_tree = any(s in model_type for s in [\"randomforest\", \"xgb\", \"decisiontree\", \"extratrees\"])\n",
    "\n",
    "            # ---------------- SHAP ----------------\n",
    "            sv_arr = None\n",
    "            if is_tree:\n",
    "                try:\n",
    "                    sv_arr = compute_shap_2d_tree(clf, X_bg_np, X_ts_np, pos_idx_local)\n",
    "                except Exception as e_tree:\n",
    "                    warnings.warn(f\"[INFO] {model_name} rep={rep}: TreeSHAP failed ({e_tree}); using KernelSHAP.\")\n",
    "\n",
    "            if sv_arr is None:\n",
    "                K = min(100, max(20, X_bg_np.shape[0] // 2))\n",
    "                try:\n",
    "                    bg_kernel = shap.kmeans(X_bg_np, K)\n",
    "                except Exception:\n",
    "                    idx = rng.choice(X_bg_np.shape[0], size=min(K, X_bg_np.shape[0]), replace=False)\n",
    "                    bg_kernel = X_bg_np[idx]\n",
    "\n",
    "                def proba_fn_sel(z):\n",
    "                    return clf.predict_proba(np.asarray(z))[:, pos_idx_local]\n",
    "\n",
    "                explainer = shap.KernelExplainer(proba_fn_sel, bg_kernel, link=\"logit\")\n",
    "                sv_arr = explainer.shap_values(X_ts_np, nsamples=1024)\n",
    "                if isinstance(sv_arr, list):\n",
    "                    sv_arr = sv_arr[0]\n",
    "\n",
    "            # Trim mismatch\n",
    "            if sv_arr.shape[1] != len(feat_names_sel_clean):\n",
    "                feat_names_sel_clean = feat_names_sel_clean[:sv_arr.shape[1]]\n",
    "\n",
    "            # ---------------- SHAP PLOTS (Clinical naming)\n",
    "            try:\n",
    "                shap.summary_plot(\n",
    "                    sv_arr,\n",
    "                    feature_names=feat_names_sel_clean,\n",
    "                    plot_type=\"bar\",\n",
    "                    show=False,\n",
    "                    max_display=20\n",
    "                )\n",
    "                fig = plt.gcf()\n",
    "                fig.tight_layout()\n",
    "                fig.savefig(\n",
    "                    EXPL_DIR / f\"{safe_model}__rep{rep:02d}__shap_bar_pediatric_iih.png\",\n",
    "                    dpi=150, bbox_inches='tight'\n",
    "                )\n",
    "                plt.close(fig)\n",
    "            except Exception as e:\n",
    "                warnings.warn(f\"[WARN] {model_name} rep={rep} SHAP bar error: {e}\")\n",
    "\n",
    "            try:\n",
    "                shap.summary_plot(\n",
    "                    sv_arr,\n",
    "                    X_ts_np,\n",
    "                    feature_names=feat_names_sel_clean,\n",
    "                    show=False,\n",
    "                    max_display=20\n",
    "                )\n",
    "                fig = plt.gcf()\n",
    "                fig.tight_layout()\n",
    "                fig.savefig(\n",
    "                    EXPL_DIR / f\"{safe_model}__rep{rep:02d}__shap_beeswarm_pediatric_iih.png\",\n",
    "                    dpi=150, bbox_inches='tight'\n",
    "                )\n",
    "                plt.close(fig)\n",
    "            except Exception as e:\n",
    "                warnings.warn(f\"[WARN] {model_name} rep={rep} SHAP beeswarm error: {e}\")\n",
    "\n",
    "            # ---------------- Dependence plots\n",
    "            try:\n",
    "                mean_abs = np.mean(np.abs(sv_arr), axis=0)\n",
    "                top_idx = np.argsort(mean_abs)[::-1][:min(TOPK_DEP, len(mean_abs))]\n",
    "                for j in top_idx:\n",
    "                    shap.dependence_plot(\n",
    "                        ind=int(j),\n",
    "                        shap_values=sv_arr,\n",
    "                        features=X_ts_np,\n",
    "                        feature_names=feat_names_sel_clean,\n",
    "                        interaction_index=None,\n",
    "                        show=False\n",
    "                    )\n",
    "                    fig = plt.gcf()\n",
    "                    fig.tight_layout()\n",
    "                    fig.savefig(\n",
    "                        EXPL_DIR / f\"{safe_model}__rep{rep:02d}__shap_dependence_feat{int(j)}_pediatric_iih.png\",\n",
    "                        dpi=150, bbox_inches='tight'\n",
    "                    )\n",
    "                    plt.close(fig)\n",
    "            except Exception as e:\n",
    "                warnings.warn(f\"[WARN] {model_name} rep={rep} dependence error: {e}\")\n",
    "\n",
    "            # ---------------- LIME ----------------\n",
    "            try:\n",
    "                lime_expl = LimeTabularExplainer(\n",
    "                    training_data=X_bg_np,\n",
    "                    feature_names=feat_names_sel_clean,\n",
    "                    class_names=CLINICAL_CLASS_NAMES,        # <--- FIXED HERE\n",
    "                    mode='classification',\n",
    "                    discretize_continuous=True\n",
    "                )\n",
    "\n",
    "                def predict_fn_lime(data):\n",
    "                    data = np.asarray(data)\n",
    "                    return clf.predict_proba(data)\n",
    "\n",
    "                for i in range(min(3, X_ts_np.shape[0])):\n",
    "                    exp = lime_expl.explain_instance(\n",
    "                        X_ts_np[i],\n",
    "                        predict_fn_lime,\n",
    "                        num_features=min(X_ts_np.shape[1], 10),\n",
    "                        labels=[pos_idx_local]\n",
    "                    )\n",
    "                    html = exp.as_html(\n",
    "                        labels=[pos_idx_local]\n",
    "                    )\n",
    "                    with open(\n",
    "                        LIME_DIR / f\"{safe_model}__rep{rep:02d}__lime_idx{i}_pediatric_iih.html\",\n",
    "                        \"w\", encoding=\"utf-8\"\n",
    "                    ) as f:\n",
    "                        f.write(html)\n",
    "\n",
    "            except Exception as e:\n",
    "                warnings.warn(f\"[WARN] {model_name} rep={rep} LIME error: {e}\")\n",
    "\n",
    "            print(f\"[OK] {model_name} rep={rep}: SHAP & LIME saved (Pediatric IIH clinical labels).\")\n",
    "\n",
    "print(\"\\nAll explainability steps completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60fd345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 12) Selected features + frequency + best hyperparameters (LONG/WIDE)\n",
    "#         Pediatric IIH vs Control — FULLY UPDATED VERSION\n",
    "# ============================================================\n",
    "\n",
    "SEL_DIR = BASE_OUT_DIR / f\"selected_features_{RUN_TAG}_pediatric_iih\"\n",
    "SEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def get_feature_names_after_prep_only(pipe, X_ref):\n",
    "    \"\"\"\n",
    "    Since 'prep' is now a Pipeline (impute + scale),\n",
    "    it does NOT support get_feature_names_out.\n",
    "    Therefore, safest approach is to use original feature names.\n",
    "    \"\"\"\n",
    "    names = list(X_ref.columns)\n",
    "    return names, names  # clean names = same names since all numeric\n",
    "\n",
    "def safe_array(a, target_len):\n",
    "    if a is None:\n",
    "        return np.array([np.nan] * target_len, dtype=float)\n",
    "    a = np.asarray(a)\n",
    "    L = min(len(a), target_len)\n",
    "    return np.asarray(a[:L], dtype=float)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# (1) LONG TABLE OF SELECTED FEATURES PER RUN\n",
    "# -------------------------------------------------------------------------\n",
    "rows = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    for rep, pipe in (final_pipes.get(model_name, {}) or {}).items():\n",
    "        \n",
    "        # PREP-SAFE feature names\n",
    "        names_trans, names_clean = get_feature_names_after_prep_only(pipe, X)\n",
    "\n",
    "        feat = pipe.named_steps.get(\"feat\", None)\n",
    "        if feat is not None and hasattr(feat, \"get_support\"):\n",
    "            mask   = feat.get_support()\n",
    "            scores = getattr(feat, \"scores_\", None)\n",
    "            pvals  = getattr(feat, \"pvalues_\", None)\n",
    "        else:\n",
    "            mask   = np.ones(len(names_clean), dtype=bool)\n",
    "            scores = None\n",
    "            pvals  = None\n",
    "\n",
    "        L = len(names_clean)\n",
    "        mask   = mask[:L]\n",
    "        scores = safe_array(scores, L)\n",
    "        pvals  = safe_array(pvals, L)\n",
    "\n",
    "        # Ranking\n",
    "        score_for_rank = np.nan_to_num(scores, nan=-np.inf)\n",
    "        order = np.argsort(-score_for_rank)\n",
    "        rank_map = {int(idx): int(r+1) for r, idx in enumerate(order)}\n",
    "\n",
    "        k_selected = int(np.sum(mask))\n",
    "\n",
    "        for i in range(L):\n",
    "            cname = names_clean[i]\n",
    "            idx_in_X = X.columns.get_loc(cname) if cname in X.columns else np.nan\n",
    "\n",
    "            rows.append({\n",
    "                \"model\": model_name,\n",
    "                \"rep\": int(rep),\n",
    "                \"k_selected\": k_selected,\n",
    "                \"feat_pos_after_prep\": i,\n",
    "                \"feat_idx_in_X\": int(idx_in_X) if idx_in_X == idx_in_X else np.nan,\n",
    "                \"feature\": cname,\n",
    "                \"selected\": bool(mask[i]),\n",
    "                \"score_f_classif\": float(scores[i]) if scores[i] == scores[i] else np.nan,\n",
    "                \"p_value\": float(pvals[i]) if pvals[i] == pvals[i] else np.nan,\n",
    "                \"rank_desc_score\": rank_map.get(i, np.nan),\n",
    "            })\n",
    "\n",
    "df_selected_long = (\n",
    "    pd.DataFrame(rows)\n",
    "      .sort_values([\"model\",\"rep\",\"selected\",\"rank_desc_score\"],\n",
    "                   ascending=[True, True, False, True])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "out_long = SEL_DIR / \"selected_features_pediatric_iih_long.csv\"\n",
    "df_selected_long.to_csv(out_long, index=False)\n",
    "print(f\"[OK] Selected-features (long): {out_long}\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# (2) SELECTION FREQUENCY SUMMARY PER MODEL\n",
    "# -------------------------------------------------------------------------\n",
    "freq_rows = []\n",
    "\n",
    "if not df_selected_long.empty:\n",
    "    for (model_name, feat), g in df_selected_long.groupby([\"model\", \"feature\"]):\n",
    "        n_runs = g[\"rep\"].nunique()\n",
    "        n_sel  = int(g.loc[g[\"selected\"], \"rep\"].nunique())\n",
    "        freq_rows.append({\n",
    "            \"model\": model_name,\n",
    "            \"feature\": feat,\n",
    "            \"n_runs\": n_runs,\n",
    "            \"n_selected\": n_sel,\n",
    "            \"selection_rate\": (n_sel / n_runs) if n_runs > 0 else np.nan,\n",
    "            \"mean_score_selected\": g.loc[g[\"selected\"] & g[\"score_f_classif\"].notna(),\n",
    "                                         \"score_f_classif\"].mean(),\n",
    "            \"median_rank_among_all\": g[\"rank_desc_score\"].median()\n",
    "        })\n",
    "\n",
    "df_freq = (\n",
    "    pd.DataFrame(freq_rows)\n",
    "      .sort_values([\"model\",\"selection_rate\",\"mean_score_selected\"],\n",
    "                   ascending=[True, False, False])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "out_freq = SEL_DIR / \"selection_frequency_pediatric_iih.csv\"\n",
    "df_freq.to_csv(out_freq, index=False)\n",
    "print(f\"[OK] Selection frequency: {out_freq}\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# (2B) TOP-15 PLOTS\n",
    "# -------------------------------------------------------------------------\n",
    "PLOT_DIR = SEL_DIR / \"plots\"\n",
    "PLOT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for m in model_names:\n",
    "    sub = df_freq[df_freq['model'] == m].copy()\n",
    "    if sub.empty:\n",
    "        continue\n",
    "    sub = sub.sort_values('selection_rate', ascending=False).head(15)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(data=sub, x='selection_rate', y='feature')\n",
    "    plt.xlabel('Selection Rate (20 runs)')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title(f\"Top-15 Selected MRI Features — {m} — Pediatric IIH Classification\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(PLOT_DIR / f\"top_features_{m.replace(' ','_')}_pediatric_iih.png\", dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# (3) BEST HYPERPARAMETERS: LONG + WIDE\n",
    "# -------------------------------------------------------------------------\n",
    "def _flatten_dict(d, parent_key=\"\"):\n",
    "    items = []\n",
    "    if d is None:\n",
    "        return items\n",
    "    for k, v in d.items():\n",
    "        key = f\"{parent_key}.{k}\" if parent_key else str(k)\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(_flatten_dict(v, key))\n",
    "        else:\n",
    "            items.append((key, v))\n",
    "    return items\n",
    "\n",
    "hp_long_rows = []\n",
    "hp_wide_rows = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    for rec in best_hyperparameters.get(model_name, []):\n",
    "        rep = rec.get(\"rep\", np.nan)\n",
    "        bp    = rec.get(\"best_params\", {}) or {}\n",
    "        bps   = rec.get(\"best_params_structured\", {}) or {}\n",
    "        kbest = rec.get(\"k_best\", None)\n",
    "        valf1 = rec.get(\"val_f1\", np.nan)\n",
    "        valap = rec.get(\"val_ap\", np.nan)\n",
    "        valau = rec.get(\"val_auc\", np.nan)\n",
    "        opttg = rec.get(\"optimize_for\", None)\n",
    "        sel_list = rec.get(\"selected_feature_names_clean\", None)\n",
    "\n",
    "        meta = {\n",
    "            \"meta.val_f1\": valf1,\n",
    "            \"meta.val_ap\": valap,\n",
    "            \"meta.val_auc\": valau,\n",
    "            \"meta.k_best\": kbest,\n",
    "            \"meta.optimize_for\": opttg,\n",
    "            \"meta.selected_features_joined\": \";\".join(map(str, sel_list)) if sel_list else None\n",
    "        }\n",
    "\n",
    "        # LONG\n",
    "        for k, v in meta.items():\n",
    "            hp_long_rows.append({\"model\": model_name, \"rep\": rep, \"param\": k, \"value\": v})\n",
    "        for k, v in sorted(_flatten_dict(bp)):\n",
    "            hp_long_rows.append({\"model\": model_name, \"rep\": rep, \"param\": f\"best_params.{k}\", \"value\": v})\n",
    "        for k, v in sorted(_flatten_dict(bps)):\n",
    "            hp_long_rows.append({\"model\": model_name, \"rep\": rep, \"param\": f\"best_params_structured.{k}\", \"value\": v})\n",
    "\n",
    "        # WIDE\n",
    "        wide_row = {\n",
    "            \"model\": model_name,\n",
    "            \"rep\": rep,\n",
    "            \"val_f1\": valf1,\n",
    "            \"val_ap\": valap,\n",
    "            \"val_auc\": valau,\n",
    "            \"k_best\": kbest,\n",
    "            \"optimize_for\": opttg,\n",
    "            \"selected_features_joined\": \";\".join(map(str, sel_list)) if sel_list else None\n",
    "        }\n",
    "        for k, v in _flatten_dict(bp, \"best_params\"):\n",
    "            wide_row[k] = v\n",
    "        for k, v in _flatten_dict(bps, \"best_params_structured\"):\n",
    "            wide_row[k] = v\n",
    "\n",
    "        hp_wide_rows.append(wide_row)\n",
    "\n",
    "df_hp_long = pd.DataFrame(hp_long_rows).sort_values([\"model\",\"rep\",\"param\"]).reset_index(drop=True)\n",
    "df_hp_wide = pd.DataFrame(hp_wide_rows).sort_values([\"model\",\"rep\"]).reset_index(drop=True)\n",
    "\n",
    "out_hp_long = SEL_DIR / \"best_hyperparameters_pediatric_iih_long.csv\"\n",
    "out_hp_wide = SEL_DIR / \"best_hyperparameters_pediatric_iih_wide.csv\"\n",
    "\n",
    "df_hp_long.to_csv(out_hp_long, index=False)\n",
    "df_hp_wide.to_csv(out_hp_wide, index=False)\n",
    "\n",
    "print(f\"[OK] Best hyperparameters (long): {out_hp_long}\")\n",
    "print(f\"[OK] Best hyperparameters (wide): {out_hp_wide}\")\n",
    "\n",
    "# JSON snapshot\n",
    "bhp_json_out = SEL_DIR / \"best_hyperparameters_pediatric_iih_with_selected.json\"\n",
    "try:\n",
    "    with open(bhp_json_out, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(best_hyperparameters, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"[OK] JSON snapshot (bhp+selected): {bhp_json_out}\")\n",
    "except Exception as e:\n",
    "    print(f\"[WARN] JSON write skipped: {e}\")\n",
    "\n",
    "print(\"\\n[REPORT]\")\n",
    "print(f\"  - Selected features (LONG): {out_long}\")\n",
    "print(f\"  - Selection frequency      : {out_freq}\")\n",
    "print(f\"  - Best HP (LONG)           : {out_hp_long}\")\n",
    "print(f\"  - Best HP (WIDE)           : {out_hp_wide}\")\n",
    "print(f\"  - JSON snapshot            : {bhp_json_out}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
